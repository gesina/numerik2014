\documentclass[ngerman,fontsize=11pt, paper=a4, parskip=false, titlepage=true, toc=bib]{scrbook}
%%options:
%	ngerman:	provides german headers etc.; default english
%	fontsize:	use 10-12pt
%	paper:		use a4
%	parskip: 	false sets to 1em
%	titlepage:	titlepage (true) or titlehead (false)
%	toc:		table of contents options
%
%
%ESSENTIAL PACKAGES
%-----------------------------------------------------------------------section-------------------------
%..................................................
%%encodings
\usepackage[T1]{fontenc}		%font-encoding WITH e.g. ö (default is OT1 with 7- instead of 8-bit)
\usepackage[utf8]{inputenc}		%input-encoding WITH e.g. ö (depends on system), better after fontenc
%
%..................................................
%%language
\usepackage{babel}				%rules typical for chosen language(s)
%
%..................................................
%%font-settings
\usepackage{lmodern}			%font: latin modern
\usepackage{microtype}			%micro-typographic optimizing, e.g. ligatures
%
%
%
%EXTRA PACKAGES
%------------------------------------------------------------------------------------------------\\
%..................................................
%%font-settings
\usepackage{scrpage2}			%KOMA pagestyles scrheadings, scrplain
%
%..................................................
%%quotes
\usepackage{csquotes}			%easy quotes with \enquote{...}
%
%..................................................
%%maths
\usepackage{amsmath}			%improves maths-sections
\usepackage{mathtools}                  %provides tools like \DeclareParedDelimiter or \DefineMathOperator
%\usepackage{amsthm}			%for proofs etc.
\usepackage{amssymb}			%further maths symbols like \square for proofs
\usepackage{dsfont}			%for \mathds{letter} to create e.g. the rational numbers symbol Q
\usepackage{stmaryrd}			%for math symbols like lightning bold
%\usepackage{mathrsfs}			%for calligraphic math symbols with \mathscr{letter}
%\usepackage{esint}				%for different sorts of integral signs like \landupint
%
%\usepackage{siunitx}			%units
%
%
%..................................................
%%pagestyle
%\usepackage{scrpage2}			%KOMA-script specialty for headings, footnotes, extends pagestyle
%\usepackage{enumitem}			%provides nicer items for enumeration, like \alpha*)
\usepackage{enumerate}
%
%..................................................
%%colors
%\usepackage{color}				%to set/define colors, predefined: white , black, red, green, blue, cyan, magenta, yellow
%
%..................................................
%%objects
%	\usepackage{graphicx}			%for inclusion of graphics with \includegraphics{name} command
%	\usepackage{placeins}			%to set barriers for floating objects with \FloatBarrier, to add to commands, list them in as options, e.g. \usepackage[section]{placeins}
%
%..................................................
%%index/bibliography
	\usepackage{makeidx}			%enables to create an index with \makeindex in head, creates *.idx file
		\makeindex					%if index is required (after {makeidx})
%	\usepackage[backend=biber]{biblatex}	%enables to print bibliography with \printbibliography, needs \bibliography{bib ﬁles}
%		\bibliography{bib}					%loads .bib-file for biber-bibliography after {biblatex}
%
%..................................................
%%(nicer)tables
	\usepackage{booktabs} 			%enables better spacing and lines in tables
%	\usepackage{multirow}			%allows option multirow for tables (centers text vertically)
%	\usepackage{multicol}			%allows option multicol for tables
%	\usepackage{tabularx}  			%table with extendable X-column
%	\usepackage{tabulary}			%table-width matches content
%
%..................................................
%%(nicer) footnotes and marginpars
%	\usepackage[outer=4.5cm, marginparwidth=5cm]{geometry}		%provides commands to manipulate the dimension of several elements; needed for marginnote (outer is space width for margins)
%	\usepackage{marginnote}				%more individual marginpars with \marginnote{text}[width]; geometry package needed
%	\usepackage{todonotes}				%fancy styled (colored etc.) marginpars with \todo[options]{text}
%	\usepackage{footmisc}				%nicer footnotes
%
%..................................................

%strike text out
\usepackage{ulem}



%hyperlinks
	\usepackage{hyperref}			%for links/hyperlinks, better load last!, options: colorlink=true (-> no boxes but colored hyperlinks), color of all link
%
%
%..................................................
%fonts
%	\usepackage{anyfontsize}		%changes the fontsize with \fontsize{size}{baselineskip}\selectfont to any size





%DEFINITIONS
%------------------------------------------------------------------------------------------------
%..................................................
%math (theorems)
%\theoremstyle{definition}
%\newtheorem{Def}{Definition}		%definition with \begin{Def}
%\newtheorem{Ax}[Def]{Axiom}		%axiom with \begin{Ax}
%\newtheorem{Satz}[Def]{Satz}		%theorem with \begin{Satz}
%\newtheorem{Prop}[Def]{Proposition}%proposition with \begin{Prop}
%\newtheorem{Lem}[Def]{Lemma}		%lemma with \begin{Lem}
%\newtheorem{Korr}[Def]{Korrolar}	%corollary with \begin{Korr}

%colors
%\definecolor{ashgrey}{rgb}{0.7, 0.75, 0.71}




%NEW COMMAMDS
%------------------------------------------------------------------------------------------------
%\newcommand{\<new command>}{<what it shall do>}
\newcommand{\R}{\mathds{R}}
\newcommand{\Rn}{\mathds{R}^{n}}
\newcommand{\Rnn}{\mathds{R}^{n\times n}}
\newcommand{\Q}{\mathds{Q}}
\newcommand{\N}{\mathds{N}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\F}{\mathds{F}}
\newcommand{\K}{\mathcal{K}}

% for the three boxes to show float numbers
\newcommand{\floatbox}[3]{ %
	\begin{array}{|c|c|c|}
		\cline{1-3} 	
		#1 & #2 & #3\\
		\cline{1-3}
	\end{array}
	}

\newcommand{\nn}[1]{\left\| #1 \right\|}


%STYLE SETTINGS
%------------------------------------------------------------------------------------------------
%\pagestyle{scrplain}				%set pagestyle to scrheadings or scrplain (instead of headings or plain)
%\clearscrplain						%clear old style (either scrplain odr scrheadings)
%\cfoot[<text for scrplain>]{<text for scrheadings>}	%any new settings for foots/ headings
\allowdisplaybreaks              %allow multipage for equations
\renewcommand{\theequation}{\thesection.\arabic{equation}}




%************************************************************************************************
\begin{document}
	
%----------------------------------------------------------------------------------------------
%FRONTMATTER
%----------------------------------------------------------------------------------------------
\frontmatter	%for book only, part for title etc.

%TITLE(PAGE)
%	\titlehead{titlehead (free)}
	\title{Skript Numerik I}
	\subtitle{bei Prof. Dr. Blank im WS14/15}
%	\subject{type}
	\author{Gesina Schwalbe}
%	\date{date}
%	\extratitle{Schmutztitel}
%	\publishers{Verlag}
%	\uppertitleback{Titelrückseitenkopf }
%	\lowertitleback{Titelrückseitenfuß}
%	\dedication{Widmung}
%	\thanks{Fußnote}
\maketitle

%---------------------------------------------------
\tableofcontents



%----------------------------------------------------------------------------------------------
%ACTUAL CONTENT
%----------------------------------------------------------------------------------------------
\mainmatter		%for book only, part for main content of document
\chapter{Einführung}\marginpar{06.10.2014}

\chapter{Lineare Gleichungssysteme: Direkte Methoden}
\label{2.1}
Sei $ A \in \R^{n\times n}$, $b \in \R^n$. Gesucht ist $x\in \R^n$ mit 
\begin{gather*}
	A\cdot x = b
\end{gather*}
Weitere Voraussetzungen sind die Existenz und Eindeutigkeit einer Lösung.
Bemerkung:
\begin{itemize}
	\item Ein verlässlicher Lösungsalgorithmus überprüft dies und behandelt alle Fälle. 
	\item Die Cramersche Regel ist ineffizient (s. Einführung).
	\item Das Inverse für $x=A^{-1}\cdot b$ aufzustellen ist ebenso ineffizient, denn es ist keine Lösung für alle $b\in \R^n$ verlangt und der Algorithmus wird evtl. instabil aufgrund vieler Operationen.
	\item [$\Rightarrow$] Invertieren von Matrizen vermeiden!!
	\item [$\Rightarrow$] Lösen des Linearen Gleichungssystems!!
\end{itemize}

\section{Gaußsches Eliminationsverfahren} \label{2.1} \index{Gaußsches Eliminationsverfahren}\index{Dreieckszerlegung}
Das Verfahren wurde 1809 von Friedrich Gauß, 1759 von Josepf Louis Lagrange beschrieben und war seit dem 1. Jhd. v. Chr. in China bekannt.

\subsection{Vorwärtselimination} \label{2.1.1}\index{Vorwärtselimination}\index{Vorwärtssubstitution}
Das Gaußverfahren gilt der Lösung eines linearen Gleichungssystems der Form
\begin{align*}
	Ax &= b
\end{align*}
mit $A=(a_{ij})_{i,j \leq n} \in K^{n\times n}$ Matrix und $b=(b_i)_{i\leq n} \in K^n$ Vektor.\\
Der zugehörige Algorithmus sieht folgendermaßen aus:
\begin{gather*}
	\begin{array}{ccccccccc}
	a_{11}x_1 &+& a_{12}x_2 &+& \cdots &+& a_{1n}x_n & = & b_1 \\
	a_{21}x_1 &+& a_{22}x_2 &+& \cdots &+& a_{2n}x_n & = & b_2 \\
	\vdots         &&        \vdots     &&              &&   \vdots       &    & \vdots \\
	a_{n1}x_1 &+& a_{n2}x_2 &+& \cdots &+& a_{nn}x_n & = & b_n \\\\
	&&&& \Arrowvert &&&& 
	\end{array} \\
\quad 	(\text{i-te Zeile}) - (\text{1. Zeile})\cdot \frac{a_{i1}}{a_11} \Rightarrow a_{i1}=0\\
\begin{array}{ccccccccc}
&&&& \Downarrow &&&&  \\\\
a_{11}x_1 &+& a_{12}x_2 &+& \cdots &+& a_{1n}x_n & = & b_1 \\
				  &+& a_{22}^{(1)}x_2 &+& \cdots &+& a_{2n}^{(1)}x_n & = & b_2^{(1)} \\
				 &&        \vdots     &&              &&   \vdots       &    & \vdots \\
														&& && && a_{nn}^{(1)}x_n & = & b_n^{(1)} \\\\
&&&& \Downarrow &&&&\\
&&&& \vdots &&&&
\end{array} 
\end{gather*}
mit
\begin{align*}
	a_{ij}^{(1)} &= a_{ij}-a_{1j}\cdot \frac{a_{i1}}{a_{11}} & \text{für }i,j = 2, \cdots, n \\
	b_i^{(1)}      &= b_i- b_1\cdot \frac{a_{i1}}{a_{11}}        & \text{für }i = 2, \cdots, n 
\end{align*}
In jedem Schritt werden die Einträge der $k$-ten Spalte analog unterhalb der Diagonalen (also $k=1, \cdots, n-1$) eliminiert:
\begin{align*}
	(\text{$i$-te Zeile})- (\text{$k$-te Zeile})\cdot\frac{a_{ik}}{a_{kk}} && \text{für } i=k+1, \cdots ,n 
\end{align*}
Die Reihe 
\begin{gather*}
			A \rightarrow A^{(1)} \rightarrow A^{(2)} \rightarrow \dotsm \rightarrow A^{(n-1)}
\end{gather*}
wird bis zum $n$-ten Schritt fortgeführt, d.h. bis eine obere Dreiecksgestalt eintritt:
\begin{align}
\nonumber
\underbrace{	\begin{pmatrix}
	a_{11} & \dotsm & \dotsm & a_{1n} \\
	             & a_{22}^{(1)} & \dotsm & a_{2n}^{(1)} \\
	             &&              \ddots  &  \vdots \\
	   0        && &                             a_{nn}^{(n-1)}
	\end{pmatrix}}_{\coloneqq R}
	\cdot
	\begin{pmatrix}
		x_1 \\
		x_2 \\
		\vdots \\
		x_n
	\end{pmatrix}
	& =
	\underbrace{\begin{pmatrix}
		b_1 \\
		b_2^{(1)} \\
		\vdots \\
		b_n^{(n-1)}
	\end{pmatrix}}_{\coloneqq z} \\
Rx &= z 	\label{II.1.1} 
\end{align}
wobei für  $i=k+1, \cdots ,n$ die Einträge wie folgt aussehen:
\begin{align}	
	l_{ik} &\coloneqq \frac{a_{ik}^{(k-1)}}{a_{kk}^{(k-1)}} \label{II.1.2} \\
	a_{ij}^{(k)} &= a_{ij}^{(k-1)} - a_{kj}^{(k-1)}\cdot l_{ik} \label{II.1.3}
				 & \text{für } j=k+1, \cdots , n\\ %
	b_i^{(k)} &= b_i^{(k-1)} -b_k^{(k-1)} \cdot   l_{ik}\label{II.1.4}\index{Vorwärtssubstitution}
\end{align}
Dieser Prozess wird \textbf{Vorwärtselimination} genannt.

\subsection{Rückwärtselimination}\label{2.1.2}
Für die Lösung des Gleichungssystems ist dann noch die \textbf{Rückwärtssubstitution} \index{Rückwärtssubstitution} nötig:
\begin{align}
	x_1 &= \frac{b_1^{(n-1)}}{a_{nn}^{(n-1)}} \label{II.1.5} \\
	x_{n-1} &=  \frac{b_{n-1}^{(n-2)}-a_{n-1,n}^{(n-1)}\cdot x_n}{a_{(n-1)(n-1)}^{(n-2)}} \label{II.1.6} \\
	x_k &= \frac{b_k^{(k-1)}-\sum_{j=k+1}^{n}a_{kj}^{(k-1)}x_j}{a_{kk}^{(k-1)}} \label{II.1.7}
\end{align}

\subsection{Vorsicht}
	Algorithmen \ref{2.1.1} und \ref{2.1.2} sind nur ausführbar, falls für die sog. \textbf{Pivotelemente $\mathbf{a_{kk}^{(k-1)}}$ } \index{Pivotelement} gilt:
	\begin{gather*}
			a_{kk}^{(k-1)} \neq 0 \quad   \text{für } k=1, \cdots , n
	\end{gather*}
	Dies ist auch für invertierbare Matrizen nicht immer gewährleistet.
	
\subsection{Weitere algorithmische Anmerkungen}	\label{2.1.4}
Matrix $A$ und Vektor $b$ sollten möglichst \textbf{nie} überschrieben werden! (Stattdessen kann eine Kopie überschrieben werden.) \\
Das Aufstellen von $A$ und $b$ ist bei manchen Anwendungen das teuerste, sie gehen sonst verloren. In \ref{2.1.1} wird das obere Dreieck von $A$ überschrieben. Dies ist möglich, da in \eqref{II.1.3} nur die Zeilen $k+1, \cdots, n$ mithilfe der $k$-ten bearbeitet werden. Am Ende steht $R$ im oberen Dreieck von $A$ und $z$ in $b$. \\
Die $l_{ik}$ werden spaltenweise berechnet und können daher anstelle der entsprechenden Nullen (in der Kopie) von $A$ gespeichert werden, d.h.:
\begin{gather}
	\widetilde{L} \coloneqq (l_{ik})  \label{II.1.8}
\end{gather}
und $R$ werden sukzessive in A geschrieben. \\
IMAGE~MISSING \\
Der Vektor $z$ und anschließend der Lösungsvektor $x$ kann in (eine Kopie von) $b$ geschrieben werden.
Wird eine neue rechte Seite $b$ betrachtet, muss \ref{II.1.1} nicht komplett neu ausgeführt werden, da sich $\widetilde{L}$ nicht ändert. Es reicht \ref{II.1.4} zu wiederholen. \\
IMAGE~MISSING \\

\subsection{Dreieckszerlegung} \label{2.1.5} \index{Dreieckszerlegung}
Die Dreieckszerlegung einer Matrix $A$ entspricht dem Verfahren aus \ref{2.1.1}, nur ohne die Zeile \eqref{II.1.4}.

\subsection{Vorwärtssubstitution} \index{Vorwärtssubstitution}
Die Vorwärtssubstitution entspricht der in \ref{2.1.4} bzw. dem Verfahren aus \ref{2.1.1} ohne die Bestimmung von $l_{ik}$ und $R$, also nur Schritt \ref{II.1.4}.

\subsection{Gauß-Eleminator zur Lösung von $Ax=b$}\index{Gauß-Eleminator}
\begin{enumerate}[1]
	\item Dreieckszerlegung
	\item Vorwärtssubstitution        $\quad b_i^{(k)} = b_i^{(k-1)} -b_k^{(k-1)} \cdot   l_{ik} $
	\item Rückwärtssubstitution      $\quad x_k = \frac{b_k^{(k-1)}-\sum_{j=k+1}^{n}a_{kj}^{(k-1)}x_j}{a_{kk}^{(k-1)}}$
\end{enumerate}

\subsection{Rechenaufwand gezählt in \enquote{flops}} \index{flops}\index{floating point operations}\index{Rechenaufwand}
\textbf{\enquote{flops} }= \textbf{f}loating \textbf{p}oint \textbf{op}eration\textbf{s} \\
MISSING

\subsection{Definition: Landau-Symbole} \index{Landau-Symbole}
MISSING

\subsection{Allgemeines zur Aufwandsbetrachtung}\index{Rechenaufwand}
MISSING

\subsection{Formalisieren des Gauß-Algorithmus} \index{Gaußsches Eliminationsverfahren} \index{LR-Zerlegung}
\marginpar{13.10.2014}

MISSING

\subsection{Lemma (Eigenschaften der $L_k$-Matrizen)} \label{2.1.12} \index{Frobeniusmatrix}
MISSING


\subsection{Satz (LR- oder LU-Zerlegung)} \index{LR-Zerlegung}\index{LU-Zerlegung}
MISSING
\index{Verfahren von Crout}

\section{Gaußsches Eliminationsverfahren mit Pivotisierung}
MISSING
\subsection{Spaltenpivotisierung (=partielle/ halbmaximale Pivotisierung)} \index{Pivotisierung!Spalten-}\index{Pivotisierung}\index{Pivotisierung!halbmaximale}\index{Pivotisierung!partielle}

\subsection{Bemerkungen}
\index{Pivotisierung!Zeilen-}
\index{Pivotisierung!vollständige}
\index{Permutationsmatrix}

\subsection{Satz: Dreieckszerlegung mit Permutationsmatrix} \label{2.2.4} 

\paragraph{Beweis}~	\marginpar[15.10.2014 \\(Fortsetzung)]{15.10.2014 \\(Fortsetzung)}
	\begin{align*}
		PA &= LR  \\
		R &= A^{(n-1)}\\& = L_{n-1}P_{\tau_{n-1}}\dots L_1P_{\tau_1}A
		\end{align*}
		\marginpar{15.10.2014}
	Da $\tau_i$ nur zwei Zahlen $\geq i $ vertauscht, ist
	\begin{align*}\index{Vorwärtselimination}
		\Pi_i  &\coloneqq \tau_{n-1} \circ \dots \tau_i \quad\text{ für } i=1,\dots (n-1) 
		\end{align*}
	eine Permutation der Zahlen $\{i,\dots, n\}$, d.h. insbesondere gilt:
	\begin{alignat*}{2}
		\Pi_i(j)&=j  & \quad &\text{ für } j=1,\dots,(i-1) \\
		\Pi_i(j)&\in \{i, \dots, n\} & &\text{~für~}j=i,\dots, n\,. \\
		P _{\Pi_{i+1}}  &= (e_1, \dotsc e_i, e_{\Pi_{i+i}(i+1)}, \dotsc, e_{\Pi_{i+1}(n)}) && \\
								&= \begin{pmatrix}
										I_i & 0 \\
										0 & P_{\sigma}
								\end{pmatrix}
	\end{alignat*}
	Damit folgt:
	\begin{align*}
	P_{\Pi_(i+1)}\cdot P_{\Pi_{i+1}}^{-1}  &= 
													P_{\Pi_{i+1}} \cdot \left(\begin{array}{ccc|ccc}
																						& I_i & && 0 & \\
																						\cline{1-6}
																						&     & -l_{i+1, i} & & & \\
																						&  0 &  \vdots      & & I_{n-i} &\\
																						&     & -l_{n, i} & &  & 
																					\end{array}\right)
																			\cdot \begin{pmatrix}
																					I_i & 0 \\
																					0 & P_{\sigma}^{-1}
																			\end{pmatrix}\\
	 &= \begin{pmatrix}
				 I_i & 0 \\
				 0 & P_{\sigma}
			 \end{pmatrix} \cdot   \l\cdot  \cdot  \left(\begin{array}{ccc|ccc}
													 & I_i & && 0 & \\
													 \cline{1-6}
												\cdot  	 &     & -l_{i+1, i} & & & \\
													 &  0 &  \vdots      & & P_{\sigma}^{-1} &\\
													 &     & -l_{n, i} & &  & 
												\end{array}\right) \\
	 &= \left(\begin{array}{ccc|ccc}
				 & I_i & && 0 & \\
				 \cline{1-6}
				 &     & -l_{\Pi{i+1}(i+1), i} & & & \\
				 &  0 &  \vdots      & & I_{n-i} &\\
				 &     & -l_{\Pi{i+1}(n), i} & &  & 
		 \end{array}\right) \\
	 &= I - (P_{\Pi_{i+1}} l_i)e_i^T\\
	 &\eqqcolon \widehat{L}_i
	\end{align*}
	und
	\begin{align*}		R =&\, L_{n-1}\\
					&\cdot (P_{\tau_{n-1}}L_{n-2}P_{\tau_{n-1}}^{-1})\\
		&				\cdot (P_{\tau_{n-1}}P_{\tau_{n-2}}L_{n-2}P_{\tau_{n-2}}^{-1}P_{\tau_{n-1}}^{-1})\\
		&\; \vdots \\
		&		 \cdot (P_{\tau_{n-1}}\dotsm P_{\tau_{1}}L_{1}P_{\tau_{1}}\dotsm P_{\tau_{n-1}}) \cdot A\\
=&\,L_{n-1}\widehat{L}_{n-2}\dotsm\widehat{L}_1P_{\Pi_{1}}\cdot A
\end{align*}
Nach Lemma \ref{2.1.12} gilt daher, es existiert eine Permutation $\Pi_{1}$ mit
\begin{gather*}
	P_{\Pi_1}\cdot A = LR ,
	\end{gather*}
wobei $R$ obere Dreiecksgestalt hat und
\begin{align*}
		L  &=  \begin{pmatrix}
						1 && & 0\\
						l_{\Pi_2(2),1} & \ddots & \\
						\vdots &            \ddots &  1\\
						l_{\Pi_n(n),1}& \dotsm &  l_{\Pi_n(n),n-1} & 1 
					\end{pmatrix} 
					& \text{mit } |l_{ij}| \leq 1 
\end{align*}
gilt.

\subsection{Lösen eines Gleichungssystems $Ax=b$} \label{2.2.5}
\subsection{Bemerkungen}
\subsection{Beispiel zur Pivotisierung}



\chapter{Fehleranalyse} \index{Fehler}
%
\begin{align*}
	\overset{x+\epsilon \text{ statt } x}{\framebox[3cm]{Eingabe}} \longrightarrow 
	\overset{\underset{\text{\tiny (z.B. durch Rundung)}}{\widetilde{f} \text{ statt } f}}{\framebox[3cm]{Algorithmus}} \longrightarrow
	\overset{\widetilde{f}(x+\epsilon) \text{ statt } f(x)}{\framebox[3cm]{Resultat\phantom{g}}}
\end{align*}\\

Bei der Fehleranalyse liegt das Hauptaugenmerk auf
\begin{itemize}
	\item[] \textbf{Eingabefehler}\\ z.B.Rundungsfehler, Fehler in Messdaten, Fehler im Modell (falsche Parameter)
	\item[] \textbf{Fehler im Algorithmus} \\ z.B. Rundungsfehler durch Rechenoperationen, Approximationen \\
	 (z.B. Ableitung durch Differenzenquotient oder die Berechnung von Sinus durch abgebrochene Reihenentwicklung)
	\\
	\item[\textit{1. Frage}] Wie wirken sich Eingabefehler auf das Resultat unabhängig vom gewählten Algorithmus aus?
		\item[\textit{2. Frage}]Wie wirken sich (Rundungs-)Fehler des Algorithmus aus?\\
											Und wie verstärkt der Algorithmus Eingabefehler?
\end{itemize}

\index{Stabilität des Algorithmus}
\section{Zahlendarstellung und Rundungsfehler} \label{3.1} \index{Fehler} \index{Rundungsfehler}\index{Zahlendarstellung}
Auf (Digital-)Rechnern können nur endlich viele Zahlen realisiert werden. \\
Die wichtigsten Typen sind: 
\begin{itemize}
	\item \textbf{ganze Zahlen}  (integer)\index{integer}:
					\begin{align*}
						 z&=\pm \sum_{i=0}^{m}z_i\beta_i & \text{mit }
						 \begin{array}{l@{\,}l}
							 \beta &= \text{Basis des Zahlensystems (oft $\beta=2$)} \\
							 z_i &\in \{0, \cdots \beta-1\}
						 \end{array}
						\end{align*}
	\item \textbf{Gleitpunktzahlen} (floating point) \index{floating point}
\end{itemize}

\subsection{Definition: Gleitkommazahl} \label{3.1.1} \index{Gleitkommazahl}
Eine Zahl $x\in\Q$ mit einer Darstellung
\begin{align*}
	x&=\sigma \cdot(a_1 . a_2 \cdots a_t)_{\beta}\cdot \beta^e 
	 = \sigma\beta^e \cdot \sum_{\nu=1}^{t}a_\nu \beta^{-\nu+1}\\\\
&\quad\begin{array}{ll}
	\beta\in\N & \text{Basis des Zahlensystems}\index{Basis}\\
	\sigma \in\{\pm 1\} &\text{Vorzeichen} \\
	m = (a_1 . a_2 \cdots a_t)_{\beta} &\text{Mantisse}\index{Mantisse}\\
	\phantom{m}= \sum_{\nu=1}^{t}a_\nu \beta^{-\nu+1} \\
	a_i \in\{0,\cdots , \beta-1\}&\text{Ziffern der Mantisse}\\
	t\in\N&\text{Mantissenlänge} \\
	e\in\Z &\text{mit }e_{min}\leq e \leq e_{max} \text{ Exponent}
\end{array}
\end{align*}
heißt \textbf{Gleitkommazahl} mit $t$ Stellen und Exponent $e$ zur Basis $b$. \\
Ist $a_1\neq 0$, so heißt $x$ \textbf{normalisierte Gleitkommazahl}\index{normalisierte Gleitkommazahl}

\subsection{Bemerkung} \label{3.1.2}
\begin{enumerate}[a)]
	\item 0 ist keine normalisierte Gleitkommazahl, da $a_1 =  0$ ist.
	\item $a_1\neq 0$ stellt sicher, dass die Gleitkommadarstellung eindeutig ist.
	\item In der Praxis werden auch nicht-normalisierte Darstellungen verwendet.
	\item Heutige Rechner verwenden meist $\beta =2$, aber auch $\beta=8, \beta=16$.
\end{enumerate}

\subsection{Beispiel} \label{3.1.3}
 bit-Darstellung nach IEEE-Standard 754 von floating point numbers \\
 Sei die Basis $\beta=2$.
 
\begin{tabular}{l@{}cccc@{}}
				& Speicherplatz & $t$ & $e_{min}$ & $e_{max}$ \\
				\cmidrule{2-5}
	einfache Genauigkeit (float) \index{floating point} & 32bits = 4Bytes & 24 &-126 & 127 \\
	doppelte  Genauigkeit (double)~~\index{double} & 64bits = 8Bytes& 52 & -1022 & 1023
\end{tabular}\\

Darstellung im Rechner (Bitmuster) für float:
\begin{gather*}
\floatbox{s}{b_0\cdots b_7}{a_2\cdots\cdots a_{24}}\\
\text{(Da $a_1\neq 0$, also $a_1=1$ gilt, wird $a_1$ nicht gespeichert)}
\end{gather*}

Interpretation ($s,b,a_i\in\{0,1\} \forall i$)
\begin{itemize}
		\item$s$ Vorzeichenbit: $\quad \sigma=(-1)^s 
										\Rightarrow \begin{array}{l}
																\sigma(0)=1 \\
																\sigma(1)=-1
															\end{array} $
		\item $b=\sum_{i=0}^{7}b_i\cdot2^i \in \{1, \cdots, 254\}$ speichert den Exponenten mit \\
							$ \quad e = b-\underbrace{127}_\text{Basiswert}$ (kein Vorzeichen nötig) \\
							Beachte: $b_0=\cdots=b_7=0$ sowie $b_0=\cdots=b_7=0$ sind bis auf Ausnahmen keine gültigen Exponenten
		\item $m=(a_1.a_2\cdots a_{24})=1+\sum_{\nu=2}^{24}a_{\nu}2^{1-\nu}$ stellt die Mantisse dar, $a_1=1$ wird nicht abgespeichert.
		\item Besondere Zahlen per Konvention:
		\begin{itemize}
			\item[$x=0$:] $s$ bel., $b=0$, $m=1 \quad \floatbox{s}{0\cdots0}{0\cdots0}$
			\item[$x=\pm\infty$:]  $s$ bel., $b=255$, $m=1  \quad \floatbox{s}{1\cdots1}{0\cdots0}$
			\item[$x=$NaN] $s$ bel., $b=255$, $m\neq 1$
			\item[$x=(-1)^s$] $s$ bel., $b=0$, $m\neq 1$ und x hat die Form $x=(0+\sum_{\nu=2}^{24}a_{\nu}\cdot 2^{1-\nu})\cdot 2^{126}$ (\enquote{denormalized} number)
		\end{itemize}
\end{itemize}

  
 \marginpar{20.10.2014}
\begin{align*}
\intertext{Betragsmäßig \textbf{größte Zahl}:}
	\floatbox{0}{01\cdots 1}{ 1\cdots\cdots 1} && 
	 x_{max} = (2-2^{-23})\cdot 2^{127}  & \approx 3,4 \cdot 10^{38}
\intertext{Betragsmäßig \textbf{kleinste Zahl}:}
	\floatbox{0}{0\cdots 0}{ 0\cdots\cdots 01} && 
	x_{min} = (2-2^{-23})\cdot 2^{-126} = 2^{-149}  & \approx 1,4 \cdot 10^{-45}
\end{align*}

\subsection{Verteilung der Maschinenzahlen} \label{3.1.4}
ungleichmäßig im Dezimalsystem, z. B.
\begin{align*}
		x &= \pm a_1 . a_2 a_3 \cdot 2^e  & -2\leq & e\leq 1 & a_i & \in \{0,1\}  \\
		IMAGE~MISSING
\end{align*}
ist im Dualsystem gleichmäßig verteilt.

\subsection{Bezeichnungen} \label{3.1.5}
\begin{itemize}
	\item[\textbf{overflow}] es ergibt sich eine Zahl, die betragsmäßig größer ist als die größte maschinendarstellbare Zahl
	\item[\textbf{underflow}] entsprechend, betragsmäßig kleiner als die kleinste positive Zahl
\end{itemize}
Bsp.: overflow beim integer $b=e+127$
\begin{align*}
	\begin{array}{rrr@{}}
	b &= 254                                &11111110 \\
	   &+  \phantom{24}3 &00000011 \\
	   \cline{3-3} %
	 b+3 = 257 \text{ mod } 2^8  &=\phantom{24}1& \xout{1}00000001 
	\end{array}	  
\end{align*}


\subsection{Rundungsfehler} \label{3.1.6}
Habe $x\in \R $ die normalisierte Darstellung
\begin{align*}
	x &= \sigma \cdot \beta^e (\sum_{\nu=1}^{t}a_{\nu}\beta^{1-\nu} + \sum_{\nu=t+1}^{\infty}a_{\nu}\beta^{1-\nu} ) \\
	  &= \sigma \cdot \beta^e (\sum_{\nu=1}^{t}a_{\nu}\beta^{1-\nu} + \beta^{1-t}\sum_{l=1}^{\infty}a_{t+l}\beta^{-l} )
\end{align*}
mit $e_{min} \leq e \leq e_{max}$, dann wird mit $fl(x)$ die gerundete Zahl bezeichnet, wobei $fl(x)$ 
eindeutig gegeben ist durch die Schranke an den \textbf{absoluten Rundungsfehler} \index{Fehler!absoluter Rundungsfehler}
\begin{align*}
	| fl(x) - x | \leq \begin{cases}
								\frac{1}{2}\beta^{e+1+t} & \text{bei symmetrischem Runden}\\
								\beta^{e+1+t}                    & \text{bei Abschneiden}
							\end{cases} \quad .
\end{align*}
Für die \textbf{relative Rechengenauigkeit} \index{Genauigkeit!relative Rechengenauigkeit}folgt somit 
\begin{align*}
\frac{| fl(x) - x | }{|x|} & \leq \begin{cases}
												\frac{1}{2}\beta^{1-t} & \text{bei symmetrischem Runden}\\
												\beta^{1-t}                    & \text{bei Abschneiden}
											\end{cases} \quad .
\end{align*}
Die \textbf{Maschinengenauigkeit} \index{Genauigkeit!Maschinengenauigkeit} des Rechners ist daher durch 
\begin{align*}
	  eps &= \beta^{1-t} & \text{(für float}\approx 10^{-7}  \text{, für double} \approx10^{-16} )
\end{align*}
gegeben.

Die Mantissenlänge bestimmt also die Maschinengenauigkeit. Bei einfacher Genauigkeit ist $fl(x)$ bis auf ungefähr 7 signifikante Stellen genau. \\
Im Folgenden betrachten wir symmetrisches Runden und definieren daher
\[ \tau \coloneqq \frac{1}{2}eps\]
Weiterhin gilt:
\begin{enumerate}[a)]
	\item Die kleinste Zahl am Rechner, welche größer als 1 ist, ist
					\[ 1 + eps \]
	\item Eine Maschinenzahl x repräsentiert eine Eingabemenge
					\[  E(x) = \{\widetilde{x} \in \R : |\widetilde{x}-x| \leq \tau|x|\} \] \\
					IMAGE~MISSING
\end{enumerate}

\subsection{Bemerkung} \label{3.1.7}
Gesetze der arithmetischen Operationen gelten i.A. nicht, z.B.
\begin{itemize}
	\item 	$x$ Maschinenzahl $\quad \Rightarrow fl(x+\nu) = x \text{     für }|\nu| < \tau |x|$
	\item Assoziativ- und Distributivgesetze gelten nicht, z.B. für $\beta = 10, \, t=3, \, a=0,1 ,\, b= 105 , \, c= -104$ gilt:
					\begin{align*}
						fl(a+fl(a+c)) &= 1,1 \\
						fl(fl(a+b)+c) &= fl(fl(105,1) + (-104) ) \\
							                &= fl(105-104) \\
							                &= 1 \quad \lightning
					\end{align*}
	\item[ $\Rightarrow$] Für einen Algorithmus ist die Reihenfolge der Operationen wesentlich!
									  Mathematisch äquivalente Formulierungen können zu verschiedenen Ergebnissen führen.
\end{itemize}

\subsection{Auslöschung von signifikanten Stellen} \label{3.1.8}
Sei $x=9,995\cdot 10^{-1}, y=9,984 \cdot 10^{-1}$. Runde auf drei signifikante Stellen und berechne $x-y$:
\begin{align*}
	\widetilde{f}(x,y) &\coloneqq fl(fl(x)- fl(y)) = fl(1,00\cdot 10^0 - 9,98\cdot 10^{-1}) \\
							  &= 	fl(0,02\cdot 10^{-1}) \\
							  &= fl(2,00 \cdot 10^{-3}) \\
	f(x,y)  &\coloneqq x-y \\
		      &\coloneqq 0,0011 = 1,1\cdot 10^{-3}
	\intertext{Daraus ergibt sich der relative Fehler}
	\frac{|\widetilde{f}(x,y)-f(x,y)|}{|f(x,y)|}
		     &= \frac{|2\cdot 10^{-3}- 1,1\cdot 10^{-3}|}{|1,1\cdot 10^{-3}}
		       = 82\%
\end{align*}
Der Grund hierfür ist, dass das Problem der Substraktion zweier annähernd gleich großer Zahlen
schlecht konditioniert ist.\\

\textbf{Zwei Regeln:}
\begin{enumerate}[1)]
	\item Umgehbare Substraktion annähernd gleich großer Zahlen vermeiden!
	\item Unumgängliche Substraktion möglichst an den Anfang des Algorithmus stellen! (siehe später)
\end{enumerate}

% 2.2
%-----------------------------------------------------------------------------------------------------------------------------------------
\section{Kondition eines Problems} \label{3.2}
Es wird das Verhältnis 
\begin{gather*}
	\frac{\text{Ausgabefehler}}{\text{Eingabefehler}}
\end{gather*}
untersucht.

\subsection{Definition: Problem} \label{3.2.1} \index{Problem}
Sei $f: U \subseteq \R^n \mapsto \R^m$ mit $U$ offen und sei $x\in U$. Dann bezeichne $(f, x)$ das Problem, zu einem gegebenen $x$ die Lösung $f(x)$ zu finden.

\subsection{Definition: absoluter und relativer Fehler} \label{3.2.2} \index{Fehler}
Sei $x\in\R^n$ und $\widetilde{x} \in \R^n$ eine Näherung an $x$. Weiterhin sei $\|\cdot\|$ eine Norm auf $\R^n$.
\begin{itemize}
	\item[a)] $\nn{\widetilde{x} - x}$ heißt \textbf{absoluter Fehler} \index{Fehler!absoluter}
	\item[b)] $\frac{\nn{\widetilde{x} - x}}{\nn{x}}$ heißt \textbf{relativer Fehler}\index{Fehler!relativer}
\end{itemize}
Da der relative Fehler skalierungsinvariant ist, d.h. unabhänging von der  Wahl von $x$ ist, ist dieser i.d.R. von größerem Interesse.
Beide Fehler hängen von der Wahl der Norm ab!
Häufig werden Fehler auch komponentenweise gemessen:
\begin{align*}
	\text{Für } i=1,\cdots , n : && |\widetilde{x}_i - x_i | & \leq \delta & \text{ (absolut)} \\
											 && |\widetilde{x}_i - x_i | &\leq \delta |x_i| & \text{ (relativ)}
\end{align*}

\subsection{Wiederholung: Normen} \label{3.2.3}\index{Norm}

\begin{align*}
	\text{Euklidische Norm ($l_2$-Norm):} &&	\nn{x}_2 &\coloneqq \sqrt{\sum_{i=1}^{n}|x_i|^2}
		\index{Norm!Euklidische Norm} \\
			IMAGE~MISSING \\
	\text{Maximumsnorm ($l_\infty$-Norm):} &&\nn{x}_\infty &\coloneqq \max\{|x_i| : i=1, \cdots n\} \\
	  	\index{Norm!Maximumsnorm}
		  	IMAGE~MISSING \\
	\text{Summennorm ($l_1$-Norm):} &&	\nn{x}_1 &\coloneqq \sum_{i=1}^{n}|x_i| 
		\index{Norm!Summennorm}\\
			IMAGE~MISSING \\
	\text{Hölder-Norm ($l_p$-Norm):} &&	\nn{x}_p &\coloneqq 
		\left(\sum_{i=1}^{n}|x_i|^p\right)^{\frac{1}{p}} 
		\index{Norm!Hölder-Norm}
\end{align*}

\subsection{Definition: Matrixnorm} \label{3.2.4}
Auf dem $\R^n$  sei die Norm $\nn{\,\cdot\,}_a$ und auf dem $\R^m$ die Norm $\nn{\,\cdot\,}_b$ gegeben.
Dann ist die zugehörige \textbf{Matrixnorm} \index{Norm!Matrixnorm} gegeben durch:
\begin{align}
	\nn{A}_{a,b} &\coloneqq \sup_{x\neq 0} \frac{\nn{Ax}_b}{\nn{x}_a} \\ \nonumber
					 &= \sup_{\nn{x}_a=1} \nn{Ax}_b \label{Matrixnorm} 
\end{align}
Also ist   $\nn{A}_{a,b}$ die kleinste Zahl $c>0$ mit
\begin{gather*}
	\nn{Ax}_b  \leq c\nn{x}_a \quad\quad \forall x\in \R^n
\end{gather*}

\subsection{Definition: Frobeniusnorm, p-Norm, Verträglichkeit} \label{3.2.5}
Sei $A\in \R^{m\times n}$.
\begin{enumerate}[a)]
	\item \textbf{Frobeniusnorm} (Schurnorm):
			 $ \quad \nn{A}_F \coloneqq \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}^2|}
				 \index{Norm!Frobeniusnorm}$
    \item \textbf{p-Norm}: 
			 $\quad \nn{A}_p \coloneqq \nn{A}_{p,p}
				 \index{p-Norm}$
    \item Eine Matrixnorm heißt \textbf{verträglich} \index{Norm!verträglich} mit den Vektornormen 
			    $\nn{\,\cdot\,}_a, \nn{\,\cdot\,}_b$, falls gilt
			    \footnote{ Beachte: $\nn{A}_{a,b}$ ist die kleinste Norm im Gegensatz zu $\nn{A}$, welche hier beliebig ist.}:
				 \begin{gather*}
					 	\nn{Ax}_b \leq \nn{A}\cdot \nn{x}_a \quad \forall x\in \R^n
				 \end{gather*}
\end{enumerate}

\subsection{Bemerkungen} \label{3.2.6}
\begin{enumerate}[a)]
	\item Die Normen $\nn{\,\cdot\,}_F$ und $\nn{\,\cdot\,}_p$ sind \textbf{submultiplikativ} \index{Norm!submultiplikative}, d.h.
				\begin{gather*}
					\nn{A\cdot B} \leq \nn{A}\cdot\nn{B}
				\end{gather*}
	\item Die Norm $\nn{\,\cdot\,}_{1,1}$ wird auch \textbf{Spaltensummennorm}\index{Norm!Spaltensummennorm} genannt:
				\begin{gather*}
					\nn{A}_1 = \max_{1\leq j \leq n}\sum_{i=1}^{m}|a_{ij}|
				\end{gather*}
			Sie ist das Maximum der Spaltensummen\footnote{Beweis: siehe Übungsblatt 3}.
	\item Die Norm $\nn{\,\cdot\,}_{\infty, \infty}$ wird auch \textbf{Zeilensummennorm} \index{Zeilensummennorm}
	 genannt\footnote{Beweis: siehe Übungsblatt 3}:
	%not sure, why \footref won't work here ...
				\begin{gather*}
					\nn{A}_\infty = \max_{1\leq i \leq m}\sum_{j=1}^{n}|a_{ij}|
				\end{gather*}
	\item Die Frobeniusnorm $\nn{\,\cdot\,}_F$ ist verträglich mit der euklidischen Norm $\nn{\,\cdot\,}_2$
	\item Die Wurzeln aus den Eigenwerten von $A^TA$ heißen \textbf{Singulärwerte $\sigma_i$} \index{Singulärwert} von A.
	Mit ihnen kann die $\nn{\,\cdot\,}_{2,2}$ Norm dargestellt werden\footnote{Beweis: siehe Übungsblatt 3}:
				\begin{align*}
					\nn{A}_2 &= max \{\sqrt{\mu} : A^TA\cdot x = \mu x\text{ für ein }x\neq 0 \} \\
								& = \sigma_{max}
				\end{align*}
\end{enumerate}

%ultimate evil hack to go along with numeration
\minisec{\Large3.2 a) Normweise Konditionsanalyse} \label{3.2a}\vspace{1eM}

%%Alternative to add it to table of contents:
%\renewcommand{\thesubsection}{\thesection.a)}
%\setkomafont{subsection}{\Large}
%\subsection{NORMWEISE KONDITIONSANALYSE}
%\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
%\addtocounter{subsection}{-1}
%\setkomafont{subsection}{\large}

\subsection{Definition: absolute und relative normweise Kondition}\index{normweise Kondition}
Sei $(f,x)$ ein Problem mit $f:U\subset \R^n \rightarrow \R^m$
und $\nn{\,\cdot\,}_a$ auf $\R^n$ und $\nn{\,\cdot\,}_b$ auf $\R^m$ eine Norm.
\begin{enumerate}[a)]
	\item Die \textbf{absolute normweise Kondition}\index{Kondition!absolute normweise} eines Problems $(f,x)$ ist die kleinste Zahl 
			 $\kappa _{abs} > 0 $ mit
			 \begin{align}
			 	\nn{f(\widetilde{x})-f(x)}_b &\leq \kappa _{abs}(f,x) \nn{\widetilde{x}-x}_a
			 	+ o\left(\nn{\widetilde{x}-x}_a\right) \label{III.2.2} \\\nonumber
			 	\Bigl(f(\widetilde{x})- f(x) 
						 	&=\underbrace{ f'(x)(\widetilde{x}-x)\pm o\left(\nn{\widetilde{x}-x}\right)}_{Taylorentwicklung}
							 \quad \text{für }\widetilde{x}\rightarrow x 
						 	\Bigr)
			 \end{align}
	\item Die \textbf{relative normweise Kondition}\index{Kondition!absolute normweise} eines Problems $(f,x)$  mit $x\neq 0, f(x) \neq 0$
			 	ist die kleinste Zahl 
			 	$\kappa _{rel} > 0 $ mit
			 	\begin{align}
			 \frac{	\nn{f(\widetilde{x})-f(x)}_b }{\nn{f(x)}_b}
				 &\leq \kappa _{rel}(f,x)\frac{ \nn{\widetilde{x}-x}_a}{\nn{x}_a}
			 	+ 
			 	o\left(\frac{\nn{\widetilde{x}-x}_a}{\nn{x}_a}\right) \label{III.2.3}
					 &&	\text{für } \widetilde{x} \rightarrow x
			 	\end{align}
	\item Sprechweise:
		\begin{itemize}\index{Kondition!gut/schlecht konditioniert}
			\item falls $\kappa$ \enquote{klein} ist, ist das Problem \enquote{gut konditioniert}
			\item falls $\kappa$ \enquote{groß} ist, ist das Problem \enquote{schlecht konditioniert}
		\end{itemize}
\end{enumerate}

\subsection{Lemma} \label{3.2.8}
Falls $f$ differenzierbar ist, gilt
\begin{gather}
	\kappa_{abs}(f,x) = \nn{Df(x)}_{a,b} \label{III.2.4}
\end{gather}
und für $f(x) \neq 0$
\begin{gather}
	\kappa_{rel}(f,x) = \frac{\nn{x}_a}{\nn{f(x)}_b}\cdot \|Df(x)\|_{a,b} \label{III.2.5}
\end{gather}
wobei $Df(x)$ die Jakobi-Matrix bezeichnet.
%
\subsection{Beispiel: Kondition der Addition}\label{3.2.9} \index{Kondition!Kondition der Addition}
$f(x_1, x_2) \coloneqq x_1 +x_2 , \, f:\R^2 \rightarrow \R$. \\
Wähle $l_1$-Norm auf $\R^2$ (und $\R$)
\begin{align*}
			Df(x_1, x_2) \, =(\nabla f^T) \, &= (\frac{\partial}{\partial x_1}f, \frac{\partial}{\partial x_2}f )\\
				&= (1,1) && \text{(Matrix!)}
\end{align*}
damit
\begin{align*}
	\kappa_{abs} (f,x)&= \nn{Df(x)}_{1,1} && \text{(Matrix-Norm!!)}\\
						 		&= \nn{Df(x)}_1 \\
								&=1 \\
	\kappa_{rel} (f,x) &= \frac{\nn{x}_1}{\nn{f(x)}_1} \cdot \nn{Df(x)}_{1} \\
								&= \frac{|x_1| + |x_2|}{|x_1+x_2|}
\end{align*}
Daraus folgt: Die Addition zweier Zahlen mit gleichem Vorzeichen ergibt
\begin{gather*}
	\kappa_{rel} = 1
\end{gather*}
Die Subtraktion zweier annähernd gleich großer  Zahlen ergibt eine sehr schlechte relative
Konditionierung:
\begin{gather*}
\kappa_{rel} \gg 1
\end{gather*}
Zum Beispiel in \ref{3.1.8}: Es ist 
\begin{align*}
	x &= \begin{pmatrix}
		9,995 \\
		-9,984
	\end{pmatrix}
	\cdot 10^{-1} \\
	\widetilde{x} = fl(x) &= \begin{pmatrix}
		1 \\
		-9,98\cdot 10^{-1}
	\end{pmatrix}
\intertext{also}
	\frac{|f(\widetilde{x})-f(x)|}{|f(x)|}	&= \frac{0,9}{1,1} 
															= 0,\overline{81} \\
															&\leq \kappa_{rel}(f,x)\cdot \frac{\|\widetilde{x}-x\|_1}{\|x\|_1} \\
															&= \kappa_{rel}(f,x) \cdot 4,6\cdot 10^{-4}
\end{align*}
%

\subsection{Beispiel: Lösen eines Gleichungssystems}
Sei $A\in \R^{n\times n}$ invertierbar und $b\in \R^n$. Es soll 
\begin{gather*}
	Ax =b
\end{gather*}
gelöst werden.
Die möglichen Lösungen in $A$ und in $b$ lassen sich folgendermaßen ermitteln:
\begin{enumerate}[a)]
	\item Betrachte die Störungen in $b$:\\
			Sei hierzu
			\begin{gather*}
			f: b\mapsto x= A^{-1}b 
			\end{gather*}
			Berechne dann $ \kappa(f,b)$ und löse 
			\begin{align*}
					A(x + \Delta x) &= b+\Delta b \\
					f(b + \Delta b) - f(b) &= \Delta x \\
													&= A^{-1} \cdot \Delta b && \text{da }x = A^{-1}b \\
					\Rightarrow \|\Delta x\|_{b}  &= \|A^{-1}\Delta b\|_{b} \\
														&\leq \|A^{-1}\|_{a,b}\cdot \|\Delta b\|_{b} && \forall b, \Delta b 
			\end{align*}
			wobei $\|\cdot\| $ auf $\Rnn$ die dem $\Rn$ zugeordnete Matrix-Norm sei. \\
			Die Abschätzung ist \textbf{scharf}\index{scharf}, d.h. es gibt ein $\Delta b\in \R^n$, so dass \enquote{=} gilt, nach Definition \ref{3.2.4}. \\
			Also gilt\footnote{vgl. auch Lemma \ref{3.2.8}: $\kappa_{abs}(f,b)=\nn{Df(b)}_{a,b}=\nn{A^{-1}}_{a,b}$}:
			\begin{gather}
				\kappa_{abs}(f,b) = \nn{A^{-1}}_{a,b} \label{II.2.4}
			\end{gather}
			unabhängig von b.  $ \quad \left( x\mapsto Ax \quad \kappa_{abs}\right)$\\
			Ebenso folgt die scharfe Abschätzung 
			\begin{align}
				\nonumber
				\frac{\|	f(b + \Delta b) - f(b)\|}{\|f(b)\|} &= \frac{\nn{\Delta x}}{\nn{x}}\\ \nonumber
						& = \frac{\nn{A^{-1}\Delta b}}{\nn{x}} \\ \nonumber
						& \leq  \frac{\|A^{-1}\|\cdot \|b\|}{\|x\|} \cdot \frac{\|\Delta b\|}{\|b\|} \\ \nonumber
				\intertext{Damit}
				\kappa_{rel} (f,b) &= \|A^{-1} \| \cdot \frac{\|b\|}{\|A^{-1}\cdot b\|} \label{III.2.7}
			\end{align}
			Da $\|b\| \leq \|A\|\cdot\|x\| = \|A\|\cdot \|A^{-1}b\|$ folgt:
			\begin{gather}
				\kappa_{rel}(f,b) \leq \|A\| \cdot \|A^{-1}\| \label{III.2.8}
			\end{gather}
			für alle (möglichen rechten Seiten) $b $.\\
			\ref{3.2.8} ist scharf in dem Sinne, dass es ein $\widehat{b}\in \R^n$ gibt 
			mit 
			\begin{gather*}
				\|\widehat{b}\| = \nn{A}\cdot \nn{\widehat{x}}
			\end{gather*}
			und somit
			\begin{gather}
				\kappa_{rel}(f,\widehat{b}) = \nn{A}\cdot \| A^{-1}\|
			\end{gather} %
			%
	\item Betrachte die Störungen in $A$:\\
			Löse also 
			\begin{gather*}
				(A+\Delta A)(x+\Delta x) = b
			\end{gather*}
			Sei hierzu
			\begin{align*}
				f:A&\mapsto x= A^{-1}b \\
				\R^{n\times n}&\rightarrow \R^n
			\end{align*}
			und berechne $\kappa(f,A)$ mittels Ableitung $Df(A):\R^{n\times n} \rightarrow \R^n$:
			\begin{align*}
				C\mapsto Df(A) C&= \frac{d}{dt} \left((A+tC)^{-1} \cdot b\right) \Big\vert_{t=0} \\
										  & = \frac{d}{dt}\left((A+tC)^{-1}\right)\Big\vert_{t=0}\cdot b
			\end{align*}			
			Weiterhin gilt
			\begin{align*}
				\frac{d}{dt} \left((A+tC)^{-1}\right) \Big\vert_{t=0} &= -A^{-1}CA^{-1}, \label{III.2.9}
			\end{align*}
			da
			\begin{align*}
				0&= \frac{d}{dt}I \\
				  &= \frac{d}{dt}\left( (A+tC)(A+tC)^{-1}\right)\\
				  &= C(A+tC)^{-1} +(A+tC)\cdot \frac{d}{dt}(A+tC)^{-1} \\
				  \Leftrightarrow \frac{d}{dt} (A+ tC)^{-1} 
				  &= -(A+tC)^{-1} \cdot C(A+tC)^{-1} \, ,
			\end{align*}
			falls $(A+tC)$ invertierbar ist. Für ein genügend kleines $t$ ist das gewährleistet, da $A$ invertierbar ist (s. Lemma \ref{3.2.12}).
			\begin{gather*}
				\Rightarrow Df(A) C = -A^{-1}CA^{-1}b
			\end{gather*}
			Somit folgt
			\begin{align}
				\nonumber
				\kappa_{abs} (f,A) &= \|Df(A)\| \\ \nonumber
											  &= \sup_{\substack{
																C\neq 0 \\ 
																C\in \R^{n\times n}											  	
														  	}}
														  \frac{\|A^{-1}CA^{-1}b\|}{\|C\|} \\ \nonumber
											  &\leq \sup_{\substack{
															 	C\neq 0 \\ 
															 	C \in \R^{n\times n}														  	
															 }}
												 \frac{\|A^{-1}\|\cdot\|C\|\cdot\|A^{-1}b\|}{\|C\|} \\ \nonumber
											  &= \|A^{-1}\| \cdot\|x\| \\ \nonumber
											  &\leq   \|A^{-1}\|^2 \cdot\|b\| \\ \nonumber
				 \kappa_{rel}(f,A)  &= \frac{\|A\|}{\|f(A)\|} \cdot \|Df(A)\| \\
											 &\leq \|A\|\cdot \|A^{-1}\| \label{III.2.10}
			\end{align}
	 \item betrachte Störungen in $A$ und $b$ :
		 \begin{gather*}
		 	(A+\Delta A)(x+\Delta x) = (b+\Delta b) 
		 \end{gather*}
		 Für $\kappa$ müsste $\|(A,b)\|$ festgelegt werden. Dies wird jedoch nicht betrachtet. Es gilt aber folgende Abschätzung für invertierbare Matrizen $A\in \Rnn $ und Störungen
		 $\Delta A \in \R^{n\times n}$ mit $\|A^{-1}\|\cdot \|\Delta A\| < 1$:
		 \begin{align}
			 \frac{\|\Delta x\|}{\|x\|} & \leq \|A\| \cdot \|A^{-1}\|\cdot (1- \|A^{-1}\|\cdot \|\Delta A\|) 
													 \cdot
													 \underbrace{\left(  \frac{\|\Delta b\|}{\|b\|} +  \frac{\|\Delta A\|}{\|A\|}  \right)}_{\neq  \frac{\|(\Delta A, \Delta b)\|}{\|(A,b)\|} }
													 \label{III.2.11}
		 \end{align}
		 \paragraph{Beweis:} s. Übungsblatt
\end{enumerate}

\subsection{Definition: Kondition einer Matrix} \index{Kondition!Matrix}
Sei $\|\cdot\|$ eine Norm auf $\R^{n\times n} $ und $A\in \R^{n\times n}$ eine reguläre Matrix.
Die Größe
\begin{gather*}
	\kappa_{\|\cdot\|}(A) = cond_{\|\cdot\|} \coloneqq \|A\| \cdot \|A^{-1}\|
\end{gather*}
heißt \textbf{Kondition der Matrix} bzgl. der Norm ${\|\cdot\|}$. \\
Ist  ${\|\cdot\|}$ von einer Vektor-Norm ${\|\cdot\|}_p$ induziert, bezeichnet 
	$cond_p(A)$
die $cond_{\|\cdot\|_p}(A)$. Wir schreiben $cond(A)$ für $cond_2(A)$. \\
$cond_{\|\cdot\|}(A) $ schätzt die relative Kondition eines linearen GLS $Ax=b$ für alle möglichen 
Störungen in $b$ oder in $A$ ab und diese Abschätzung ist scharf. \\

Es stellt sich nun die Frage: \\
\textit{Wann existiert die Inverse der gestörten invertierbaren Matrix $A$?} \\
Hierzu werden wir die Relationen benötigen:
\begin{align*}
	A+\Delta A &= A (I+A^{-1}\Delta A)\\
\intertext{und mit $C \in \Rnn,\, \|C\| < 1$}
	(I-C)^{-1} &= \sum_{k=0}^{\infty}C^k \\
	\|	(I-C)^{-1} \| &\leq \frac{1}{1-\|C\|}
\end{align*}

\marginpar{27.10.2014}
\subsection{Lemma (Neumannsche Reihe)}\index{Neumannsche Reihe}\label{3.2.12}
\addtocounter{equation}{1}
Sei $C\in\Rnn$ mit $\|C\|<1$ und mit einer submultiplikativen Norm $\|\cdot\|$,
so ist $(I-C)$ invertierbar und es gilt:
\begin{gather*}
	(I-C)^{-1}=\sum_{k=0}^{\infty}C^k
\end{gather*}
Weiterhin gilt:
\begin{gather*}
	\|(I-C)^ {-1}\| \leq \frac{1}{1-\|C\|}
\end{gather*}

\paragraph{Beweis}
Es gilt zu zeigen, dass $\sum_{k=1}^{\infty}C^k$ existiert: \\
Sei $q\coloneqq \|C\| < 1$, dann gilt: 
\begin{align*}
	\nn{ \sum_{k=0}^{m} C^k } &\leq \sum_{k=0}^{m} \nn{C^k }  && \text{Dreiecksungleichung} \\
								 &\leq \sum_{k=0}^{m}\nn{C}^k && \text{da $\nn{\,\cdot\,}$ submultiplikativ}\\
								 &=\sum_{k=0}^{m}q^k  \\
								 &= \frac{1-q^{m+1}}{1-q} \\
								 &\leq \frac{1}{1-\nn{C}} && \forall m\in \N, \text{ da } q<1 \text{ (geometr. Reihe)}
\end{align*}
Daraus folgt bereits, dass $\sum_{k=1}^{\infty}C^k$ existiert (nach Majorantenkriterium).\\
Weiter gilt dann:
\begin{align*}
	(I-C) \sum_{k=1}^{\infty}C^k &= \lim\limits_{m\rightarrow \infty}(I-C)   \sum_{k=1}^{m}C^k \\
													&= \lim\limits_{m\rightarrow \infty} (C^0-C^{m+1}) \\
													&=I  &&\square
\end{align*}
\subsection{Bemerkung}
\begin{enumerate}[a)]
	\item Für symmetrische, positiv definite Matrix $A\in \Rnn$ gilt\footnote{Beweis: siehe Übungsblatt 3}: 
	\begin{gather}
		\kappa_2(A) = \frac{\lambda_{max}}{\lambda_{min}} \label{III.2.13}
	\end{gather}
	\item Eine andere Darstellung von $\kappa(A)$ ist
	\begin{gather}
		\kappa(A) \coloneqq 
		\frac{\underset{\|x\|=1}{\max}\|Ax\|}{\underset{\|x\|=1}{\min}\|Ax\|} \in  \left[ 0, \infty \right]
			 \label{III.2.14}
	\end{gather}
	Diese ist auch für nicht invertierbare und rechteckige Matrizen wohldefiniert. \\
	Dann gilt offensichtlich:
	\item $\kappa(A) \geq 1$
	\item $\kappa(\alpha A)=\kappa(A) \quad \text{für } 0\neq\alpha\in\R$ (skalierungsinvariant)
	\item $A\neq 0$ und $A\in\Rnn $ ist genau dann singulär, wenn $\kappa(A)=\infty$. \\
			Wegen der Skalierungsinvarianz ist die Kondition zur Überprüfung der Regularität von $A$ 
			besser geeignet als die Determinante.
	\end{enumerate}
	

\subsection{Beispiel: Kondition eines nichtlinearen Gleichungssystems}
Sei $f:\Rn\rightarrow\Rn$ stetig differenzierbar und $y\in\Rn$ gegeben. \\
Löse
\begin{gather*}
	f(x) = y
\end{gather*}
Gesucht:
\begin{gather*}
	\kappa(f^{-1},y)
\end{gather*}
mit $f^{-1}$ Ausgabe und $y$ Eingabe. \\
Sei $Df(x)$ invertierbar, dann existiert aufgrund des Satzes für implizite Funktionen die inverse Funktion $f^{-1}$ lokal in einer Umgebung von $y$ mit $f^{-1}(y)=x$, sowie
\begin{gather*}
	D(f^{-1})(y) = (Df(x))^{-1}
\end{gather*}
Hiermit folgt:
\begin{align}
	\nonumber
	\kappa_{abs}(f^{-1},y) &= \|(Df(x))^{-1}\| \\
	\kappa_{rel}(f^{-1},y) &= \frac{\|f(x)\|}{\|x\|}\cdot\|(Df(x))^{-1}\|  \label{III.2.15}
\end{align}
Für skalare Funktionen $f:\R\rightarrow\R$ folgt somit:
\begin{gather*}
	\kappa_{rel}(f^{-1},y) = \frac{|f(x)|}{|x|}\cdot \frac{1}{|f'(x)|}
\end{gather*}
Falls $|f'(x)|\rightarrow 0$ ist es eine schlechte absolute Kondition. \\
Für $|f'(x)| \gg 0$ ist es eine gute absolute Kondition.\\

IMAGE~MISSING\\

Damit bedeutet eine kleine Störung in $y$ eine große Störung in $x$.


\minisec{\Large3.2 a) Komponentenweise Konditionsanalyse} \label{3.2b}\vspace{1eM} \index{Konditionsanalyse!komponentenweise}

\subsection{Beispiel}
Falls $A$ Diagonalmatrix hat, sind die Gleichungen unabhängig voneinander (entkoppelt)\index{entkoppelt}.
Die erwartete relative Kondition wäre dann -- wie bei skalaren Gleicungen -- stets gleich 1.
Ebenso sind Störungen nur in der Diagonale zu erwarten. Jedoch:
\begin{align*}
	A  &=\begin{pmatrix}
					1 & 0\\
					0 & \epsilon
				\end{pmatrix} \\
	\Rightarrow 	A^{-1}&=\begin{pmatrix}
													1 & 0\\
													0 & \epsilon^{-1}
												\end{pmatrix}\\
	\Rightarrow \kappa_\infty& = \kappa_2 = \frac{1}{\epsilon} 
													&& \text{für }0 < \epsilon \leq 1											
\end{align*}

\subsection{Definition: Komponentenweise Kondition}\index{Kondition!komponentenweise}
Sei $(f, x) $ ein Problem mit $f(x)\neq 0$ und $x=(x_i)_{i=1,\cdots , n}$ mit $x_1\neq 0 $  für alle $i=1,\cdots, n$.
Die \textbf{komponentenweise Kondition} von $(f,x) $ ist die kleinste Zahl $k_{rel}\geq 0$, so dass:
\begin{align*}
	\frac{\|f(\widetilde{x})-f(x)\|_\infty}{\|f(x)\|_\infty} 
				&\leq \kappa_{rel} \cdot \underset{i}{\max}\frac{|\widetilde{x_i}-x_i|}{|x_i|}+ o\left(\underset{i}{\max}\frac{|\widetilde{x_i}-x_i|}{|x_i|}\right) 
				&& \text{für }\widetilde{x}\rightarrow x
\end{align*}
Vorsicht:
\begin{gather*}
	\frac{\|\widetilde{x}-x\|_\infty}{\|x\|_\infty}\neq \underset{i}{\max}\frac{|\widetilde{x_i}-x_i|}{|x_i|}
\end{gather*}

\subsection{Lemma} \label{3.2.17}
Sei $f$ differenzierbar und fasse $|\cdot|$ komponentenweise auf, d.h. $|x| = \begin{pmatrix}
																																						|x_1| \\
																																						\vdots \\
																																						|x_n|
																																					\end{pmatrix}$.
Dann gilt:
\begin{gather}
	\kappa_{rel} = \frac{\|\, \|Df(x)|\cdot |x| \, \|_\infty}{\|f(x)\|_\infty} \label{III.2.16}
\end{gather}

\paragraph{Beweis}
Vergleiche seien ebenfalls komponentenweise zu verstehen. \\
Nach dem Satz von Taylor gilt: 
\begin{align*}
	f_i(\widetilde{x})-f_i(x) 
						&= \left( \frac{\partial f_i}{\partial x_i}(x), \cdots ,\frac{\partial f_i}{\partial x_n}(x) \right)
								\cdot \begin{pmatrix}
									\widetilde{x}_1-x_1 \\
									\vdots \\
									\widetilde{x}_n-x_n
								\end{pmatrix}
							+ o\left(\|\widetilde{x}-x\|\right) \\
	\Rightarrow |f_i(\widetilde{x})-f_i(x)|
						&\leq |Df(x)|
							\cdot \begin{pmatrix}
									|x_1|\cdot \frac{\widetilde{x}_1-x_1 }{|x_1|}\\
									\vdots \\
									|x_n|\cdot \frac{\widetilde{x}_n-x_n }{|x_n|}
								\end{pmatrix}
									+ o\left(\underset{i}{\max}\frac{\widetilde{x}_i-x_i }{|x_i|}\right) 
									&& \text{da $x_i$ fest und $\widetilde{x}_i\rightarrow x_i$} \\
	&\leq |Df(x)| \cdot |x| \cdot \underset{i}{\max}\frac{\widetilde{x}_i-x_i }{|x_i|}
									+o\left(\underset{i}{\max}\frac{\widetilde{x}_i-x_i }{|x_i|}\right) \\
	\Rightarrow \frac{\|f(\widetilde{x})-f(x)\|_\infty}{\|f(x)\|_\infty}
						&\leq  \frac{\|\, \|Df(x)|\cdot |x| \, \|_\infty}{\|f(x)\|_\infty}
								\cdot \underset{i}{\max}\frac{\widetilde{x}_i-x_i }{|x_i|}
								+ o\left( \underset{i}{\max}\frac{\widetilde{x}_i-x_i }{|x_i|} \right)
\end{align*}
Wähle $\widetilde{x}_i = x_j+h\cdot sign \frac{\partial f_i}{\partial x_j}(x)$ mit $h>0$,
dann gilt:
\begin{gather*}
	|Df_i(x)(\widetilde{x}-x)| = Df_i(x)(\widetilde{x}-x)
\end{gather*}
und in obiger Rechnung gilt Gleichheit. \\
Also folgt, dass
\begin{align*}
	 \frac{\|\, \|Df(x)|\cdot |x| \, \|_\infty}{\|f(x)\|_\infty} &= \kappa_{rel}  \\
	 &&& \square
\end{align*}

\subsection{Beispiel}
\begin{enumerate}[a)]
	\item Komponentenweise Kondition der Multiplikation
				\begin{align*}
					f:&\R^2 \rightarrow \R, \, f(x,y) \coloneqq x\cdot y \\
					   \Rightarrow Df(x,y) &= (y, x)  \\
					   \Rightarrow \kappa_{rel}(x,y) &= \frac{\left\| (|y|, |x|)\cdot \begin{pmatrix}
																																   	|x| \\
																																   	|y|
										-																					   	\end{pmatrix}\right\|_\infty}
										{|x\cdot y|} \\
						&= \frac{2\cdot|x|\cdot |y|}{|x\cdot y|} \\
						&= 2
				\end{align*}
	\item Komponentenweise Kondition eines linearen Gleichungssystems:\\
				Löse $Ax=b$ mit möglichen Störungen in $b$, also zu
				\begin{align*}
					f: & b\mapsto A^{-1}b \\
					\kappa_{rel} & = \frac{\| \, |A^{-1}| \cdot |b|\, \|_\infty}{\|A^{-1}b\|_\infty}
				\end{align*}
				Falls A eine Diagonalmatrix ist, folgt:
				\begin{gather*}
					\kappa_{rel}=1
				\end{gather*}
	\item Komponentenweise Kondition des Skalarproduktes:
				\begin{align*}
					\langle x,y \rangle \coloneqq \sum_{i=1}^{n}x_i y_i& = x^Ty \\
					f: \R^2 \rightarrow \R, \, f(x,y) &= \langle x,y \rangle \\
					\Rightarrow Df(x,y) &= (y^T, x^T) \\
					\kappa_{rel}  &= \frac{\left\| \,\left|(y^T, x^T)\right|\cdot\left|\begin{pmatrix}
																											x \\
																											y
																								 	\end{pmatrix}\right|\right\|_\infty }
																{\|\langle x,y\rangle\|_\infty}\\
												&= \frac{2\cdot |y^T|\cdot |x|}{|\langle x,y\rangle|} \\
												&= 2\cdot \frac{\langle |x|,|y|\rangle}{|\langle x,y\rangle|} \\
												&= 2 \cdot \frac{\cos(|x|, |y|)}{\cos(x,y)}  \\
	&&&				\text{	da  }\cos(x,y) = \frac{\langle y,x \rangle}{\|x\|_2 \cdot \|y\|_2} \, . 
				\end{align*}
				Falls $x$ und $y$ nahezu senkrecht aufeinander stehen, kann das Skalarprodukt sehr schlecht konditioniert sein. \\
				Zum Beispiel für $x=\widetilde{x} = \begin{pmatrix} 1 \\1 \end{pmatrix}$
					 und $y=\begin{pmatrix} 1+10^{-10} \\-1 \end{pmatrix},
					  \, \widetilde{y}=\begin{pmatrix} 1 \\-1 \end{pmatrix}$. \\
					  IMAGE~MISSING
\end{enumerate}

\section{Stabilität von Algorithmen} \index{Stabilität}
Bislang: Kondition eines gegebenen Problems $(f,x)$. \\
Nun stellt sich die Frage: \textit{Was passiert durch das Implementieren am Rechner? }\\
Ein \enquote{stabiler} Algorithmus sollte ein gut konditioniertes Problem nicht \enquote{kaputt machen}.\\

IMAGE~MISSING

\minisec{\Large3.3 a) Vorwärtsanalyse} \label{3.3a}\vspace{1eM}

Die Fehlerfortpflanzung durch die einzelnen Rechenschritte, aus denen die Implementierung aufgebaut ist, wird abgeschätzt.

\subsection{Bemerkung}
Für die Rechenoperationene $+,-,\, \cdot \, , \, /\,$, kurz $\nabla$, gilt:
\begin{align}
	\nonumber
	fl(a\nabla b) &= (a\nabla b)\cdot (1+\epsilon) \\
						   &= (a\nabla b) \cdot \frac{1}{1+\mu} \label{III.3.1}
\end{align}
mit $|\epsilon|, |\mu| \leq eps$.


\marginpar{29.10.2014}
\subsection{Beispiel}
Sei $f(x_1, x_2, x_3) \coloneqq \frac{x_1x_2}{x_3}$ mit Maschinenzahlen $x_i$ und $x_3\neq 0$ und sei der Algorithmus durch
\begin{gather*}
	f(x_1, x_2, x_3) = (f^{(2)} \circ f^{(1)})(x_1, x_2, x_3) 
\end{gather*}
gegeben mit 
\begin{align*}
	f^{(1)}(x_1, x_2, x_3) & = (x_1\cot x_2, x_3) && \text{und} \\
	f^{(2)}(y,z) &= \frac{y}{z}
\end{align*}
Die Implementierung $\widetilde{f}$ von $f$  beinhaltet Rundungsfehler. \\

Sei  $x=(x_1, x_2, x_3) $. Daraus folgt:
\begin{align*}
	\widetilde{f}^{(1)}(x) &= (fl(x_1\cdot x_2), x_3) \\
										& = (x_1x_2 (1+\epsilon_1), x_3)
\intertext{mit $|\epsilon_1|\leq eps$:}
	\widetilde{f}(x) &= \widetilde{f}^{(2)}(\widetilde{f}^{(1)}(x)) \\
							&= fl(f^{(2)}(x_1 x_2 (1+\epsilon_1), x_3)) \\
							&= \frac{x_1x_2(1+\epsilon_1)}{x_3}\cdot (1+\epsilon_2)  \\
							&= f(x)\cdot (1+\epsilon_1)(1+\epsilon_2)
\intertext{mit $|\epsilon_2| \leq eps$:}
	\frac{|\widetilde{f}(x) -f(x)|}{|f(x)|} &= |\epsilon_1+\epsilon_2 +\epsilon_1\cdot \epsilon_2| \\
								&\leq 2eps + eps^2
\end{align*}
Dies ist eine \enquote{worst case} Analyse, da immer der maximale Fehler angenommen wird,
und gibt i.d.R. eine starte Überschätzung des Fehlers an.
Für qualitative Aussagen sind sie jedoch unnützlich. \\
In Computersystemen stehen mehr Operationen wie $\nabla$ zur Verfügung,
die mit einer relativen Genauigkeit $eps$ realisiert werden können. \\

Daher:

\subsection{Definition: }\index{MISSING}
Eine Abbildung $\phi : U\subseteq \Rn \rightarrow \R^m$ heißt
\textbf{elementar ausführbar}\index{elementar ausführbar}, falls es 
eine elementare Operation $\widetilde{\phi}:\F^n \rightarrow \F^m$
gibt, wobei $\F$ die Menge der Maschinenzahlen bezeichne mit
\begin{gather}
	|\widetilde{\phi}_i(x)-\phi_i(x) \leq eps|\phi_i(x) | 
	\quad \forall x\in \F^n \text{ und } i=1,\cdots , m \label{III.3.2}\, .
\end{gather}
$\widetilde{\phi}$ heißt dann \textbf{Realisierung}\index{Realisierung} von $\phi$.

\paragraph{Bemerkung:}
aus \eqref{III.3.2} folgt für $1\leq p\leq \infty$:
\begin{gather}
	\nn{\widetilde{\phi}(x)-\phi(x)}_p \leq eps\nn{\phi(x)}_n 
	\quad \forall x\in\F^n \label{III.3.3}
\end{gather}


\subsection{Definition: } \index{MISSING}
Sei $f:E\subseteq \Rn \rightarrow \R^m$ gegeben.\\
Ein Tupel $\left(f^{(1)},\cdots ,f^{(l)}\right)$ mit $l\in \N$ von elementar ausführbaren
Abbildungen
\begin{gather*}
	f^{(i)}: U_1\subseteq \R^{k_i} \rightarrow U_{i+1}\subseteq \R^{k_{i+1}}
\end{gather*}
mit $k_1=n$ und $k_{l+1}=m$ heißt \textbf{Algorithmus}\index{Algorithmus} von $f$, falls
\begin{gather*}
	f=f	^{(l)}\circ \dotsc \circ f^{(1)}
\end{gather*}
Das Tupel $(\widetilde{f}1^{(1)},\cdots ,\widetilde{f}^{(l)})$ mit Abbildungen $\widehat{f}^{(i)}$, welche Realisierungen der $f^{(i)}$ sind,
heißt \textbf{Implementation}\index{Implementation} von 
$\left(f^{(1)},\dotsc ,f^{(l)}\right)$.
Die Komposition 
\begin{gather*}
		\widetilde{f}=\widetilde{f}	^{(l)}\circ \dotsc \circ \widetilde{f}^{(1)}
\end{gather*}
heißt Implementation von f. \\
Im Allgemeinen gibt es verschiedene Implementierungen einer Abbildung $f$.

\subsection{Lemma (Fehlerfortpflanzung)}\label{3.3.5} \index{Fehler!Fortpflanzung}
Sei $x\in \Rn$ und $\widetilde{x}\in \F^n$ mit $|\widetilde{x}_i-x_i|\leq eps|x_i|$ für alle 
$i=1,\cdots , n$.
Sei $\left(f^{(1)},\dotsc ,f^{(l)}\right)$ ein Algorithmus für $f$ und 
$(\widetilde{f}^{(1)},\dotsc ,\widetilde{f}^{(l)})$ eine zugehörige Implementation. \\
Mit den Abkürzungen
\begin{align*}
	x^{(j+1)} &\coloneqq f^{(j)}\circ \dotsc \circ f^{(1)} \\
	x^{(1)} &\coloneqq x
\end{align*}
und entsprechend mit $\widetilde{x}^{(j+1)}$ gilt,
falls $x^{(j+1)} \neq 0$ für alle $j=0,\dotsc , (l-1)$ und $\nn{\,\cdot\,}$ eine beliebige p-Norm ist:
\begin{align}
	\frac{\nn{\widetilde{x}^{(j+1)}-x^{(j+1)}}}{\nn{x^{(j+1)}}}
	&\leq eps \cdot \K + o\left(eps\right)
	\label{III.3.4} \\ \nonumber
	\K^{(j)}&=(1+\kappa^{(j)}+\kappa^{(j)}\cdot \kappa^{(j-1)}+ \cdots + \kappa^{(j)}\cdot \dotsm \cdot \kappa^{(1)}) \\ \nonumber
\end{align}
wobei $	\K^{(j)} \coloneqq \kappa_{rel}(f^{(j)}, x^{(j)})$ die Kondition der elementar ausführbaren Operationen $f^{(j)}$ ist.

\paragraph{Beweis}

\begin{align*}
	\frac{\nn{\widetilde{x}^{(j+1)}-x^{(j+1)}}}{\nn{x^{(j+1)}}}
				&= \frac{\nn{\widetilde{f}^{(j)}(\widetilde{x})^{(j)}-f^{(j)}(x^{(j)})}}
				{\nn{f^{(j)}(x^{(j)})}} \\
				&\leq \frac{\nn{\widetilde{f}(\widetilde{x})-f(\widetilde{x}})}{\nn{f(\widetilde{x})}}
						\cdot \frac{\nn{f(\widetilde{x})}}{\nn{f(x)}}
						+ \frac{\nn{f(\widetilde{x})-f({x}})}{\nn{f(x)}} 
						\quad\quad \text{(Index $j$ vernachlässigt)}\\
				&\leq eps \left( 1+ \frac{\nn{f(\widetilde{x})-f({x}})}{\nn{f(x)}}\right)
						+ \frac{\nn{f(\widetilde{x})-f({x}})}{\nn{f(x)}}\\
				&\overset{\text{nach \ref{III.3.3}}}{=} eps + (eps+1) \cdot 						\left(\kappa{(j)}\cdot \frac{\nn{\widetilde{x}^{(j)}-x^{(j)}}}{\nn{x^{(j)}}}\right)
						+ o\left( \frac{\nn{\widetilde{x}^{(j)}-x^{(j)}}}{\nn{x^{(j)}}}\right)
\end{align*}
Nach Voraussetzung gilt Gleichung \eqref{III.3.4}  mit $\K^{(0)}=1$ für $j=0$. \\
Für $j=1$ folgt nach Voraussetzung mit Gleichung \eqref{III.3.3}
\begin{align*}
	\frac{\nn{\widetilde{x}^{(2)}-x^{(2)}}}{\nn{x^{(2)}}}
	 & \leq eps +(eps+1) \cdot \left( \kappa^{(1)}eps+ o(eps)\right) \\
	 &= eps(1+\kappa^{(1)}) + o(eps) \\
	 &= eps\K^{(1)} + o(eps)
\end{align*}
Womit der Induktionsanfang gezeigt ist. \\
Für den Induktionsschritt von $j-1$ zu $j$:
\begin{align*}
	\frac{\nn{\widetilde{x}^{(j+1)}-x^{(j+1)}}}{x^{(j+1)}}
		& \leq eps + (1+eps)\kappa^{(j)} \left[ eps \K^{(j-1)}+ o(eps) \right] \\
		&\phantom{\leq eps+} + (1+eps) \cdot o\left( eps\cdot \K^{(j-1)} +o(eps)\right) \\
		&= eps\left(1+\kappa^{(j)}\cdot \K^{(j-1)}\right)+ o(eps)
\end{align*}
Mit $\K^{(j)} = 1+ \kappa^{(j)}\cdot \K^{(j-1)}$ folgt die Behauptung.
\hfill $\square$
\\

Hiermit folgt:

\subsection{Korollar}\label{3.3.6}
Unter der Voraussetzung von Lemma \ref{3.3.5} gilt:
\begin{gather}
	\frac{\nn{\widetilde{f}(\widetilde{x}-f(x))}}{\nn{f(x)}} \leq 
		eps\cdot \left( 1+\kappa{(l)}+ \kappa{(l)}\cdot \kappa{(l-1)}+ \dotsc
			+ \kappa{(l)}\cdot \dotsc \cdot \kappa{(1)}\right) + o(eps) 
			\label{III.3.5}
\end{gather}

\subsection{Bemerkung}
Mit Korollar \ref{3.3.6} ist offensichtlich, dass schlecht konditionierte Probleme 
zu elementar ausführbaren Abbildugnen so früh wie möglich ausgeführt werden sollten. \\
Nach Beispiel \ref{3.2.9} ist die Substraktion zweier annähernd gleicher Zahlen schlecht konditioniert.
Deshalb sollte man unvermeidbare Subtraktionen möglichst früh durchführen. \\
Allerdings hängt $\kappa^{(j)}$ nicht nur von $f^{(j)}$, sondern auch vom Zwischenergebnis $x^{(j)}$ ab, welches a priori unbekannt ist.

\subsection{Bemerkung zur Sprechweise} %\label{3.3.8}
Der Quotient 
\begin{gather}
	\frac{\overbrace{\frac{\nn{\widetilde{f}(\widetilde{x})-f(x)}}{\nn{f(x)}}}^{
				\text{Gesamtfehler}}}
		{\underbrace{\frac{\nn{\widetilde{f}(\widetilde{x})}}{\nn{f(x)}}}_{
				\scriptsize\substack{
					\text{Fehler} \\
					\text{durch} \\
					\text{Problem}
				}}
			\cdot
		 {\underbrace{\frac{\nn{\widetilde{x}-x}}{\nn{x}}}_{
		 		\scriptsize\substack{\text{Eingabe-} \\ \text{fehler}}}}}
	\label{III.3.6}
\end{gather}
gibt die \textbf{Güte des Algorithmus} \index{Güte!Algorithmus} an.
Als Stabilitätsindikator kann also 
\begin{gather}
	o\left(f, \widetilde{f}, x\right) \coloneqq \frac{\K}{\kappa_{rel}(f, x)}
	\label{III.3.7}
\end{gather}
 verwendet werden und es gilt
 \begin{gather*}
	\frac{\nn{\widetilde{f}(\widetilde{x})-f(x)}}{\nn{f(x)}}
		< \underbrace{o\left( f,\widetilde{f}, x\right) }_{
								\substack{\text{Beitrag}\\
												 \text{des} \\
												 \text{Algorithmus}}}
		   \cdot \underbrace{\kappa_{rel}(f,x)}_{
		   						\substack{\text{Beitrag} \\
		   										 \text{des} \\
		   										 \text{Problems}}}
		   	\cdot \underbrace{eps}_{\substack{\text{Rundungs-}\\\text{fehler}}}
		    + \quad o(eps)
 \end{gather*}
Falls $o( f,\widetilde{f}, x)  < 1$, dämpft der Algorithmus die Fehlerfortpflanzung der Eingabe- und Rundungsfehler und heißt \textbf{stabil}\index{Stabilität}. \\
Für $o( f,\widetilde{f}, x)  \gg 1$ heißt der Algorithmus \textbf{instabil}.



\subsection{Beispiel}
Nach Gleichung \eqref{III.3.3} gilt für die Elementaroperationen $\K\leq 1$.
Da für die Subtraktion zweier annähernd gleich großer Zahlen $\kappa_{rel}\gg 1$ gilt,
ist der Stabilitätsfaktor zweier annähernd gleich großer
Zahlen sehr klein und der Algorithmus also stabil, Falls es sich jedoch bei einer zusammengesetzten Abbildung $f=h\circ g$  bei der zweiten Abbildung $h$ um eine Subtaktion handelt, gilt
\begin{gather*}
	\K =(1+\kappa(sub)+\kappa(sub)\cdot\kappa(g))
\end{gather*}
und die Stabilität ist gefährdet.
Genauere Abschätzungen und damit genauere Indikatoren können durch komponentenweise Betrachtungen erhalten werden.


\subsection{Rückwärtsanalyse} \index{Rückwärtsanalyse}
Die Fragestellung ist nun: \\
\textit{Kann $\widetilde{f}(\widehat{x})$ als exaktes Ergebnis von einer gestörten Eingabe $\widehat{x}$ unter der exakten Abbildung $f$ aufgefasst werden?}\\
Das würde heißen
\begin{gather*}
	\exists\, \widehat{x}\in \Rn: f(\widehat{x})= \widetilde{f}(\widetilde{x}) \, .
\end{gather*}
Dann schätze den Fehler 
\begin{gather*} 
	\nn{\widehat{x}-x}
\end{gather*}
 bzw. für nicht injektive $f$
 \begin{gather*}
	\min_{\widehat{x}\in \Rn}
\left\{
 	\nn{\widehat{x}-x} 
 					\middle\vert f(\widehat{x}) = \widetilde{f}(\widetilde{x}) 
 					\right\}
 \end{gather*} 
  ab. \\
  
  IMAGE~MISSING

%
%\begin{gather*}
%\int \dotsi \int \\
%+ \dotsb + \\
%, \dotsc , \\
%\dotso
%\end{gather*}

%----------------------------------------------------------------------------------------------
%BACKMATTER
%----------------------------------------------------------------------------------------------
%\backmatter		%for book only, part for index etc.


\printindex		%only with package makeidx
%\listoffigures		
%\listoftables
%\printbibliography	%only with package biblatex


\end{document}
%**********************************************************************************************









