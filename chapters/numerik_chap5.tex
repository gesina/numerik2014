% % % % % % % % % % % % % % 
% 
% Skript zu NUMERIK I
% WS14/15
% von Prof. Dr. Blank
% Universität Regensburg
% 
% 
%	Kap. 5: Numerische Lösung nichtlinearer Gleichungssysteme
% 
% % % % % % % % % % % % % % 


\chapter{Numerische Lösung nichtlinearer Gleichungssysteme}
Beispiel:
\begin{description}
\item[linear:] $Ax=b$
\item[nichtlinear:] $f(x) = \sin(x) +x^3-4=0$
\end{description}

\sectione{Einführung}
\begin{Bspe}~
  \begin{enumerate}[1)]
  \item $f(x) = x^2-c = 0 \Leftrightarrow x= \pm \sqrt{c}$: Berechnung der Wurzel
  \item Sei $p$ ein Polynom: Nullstellenbestimmung
  \item Löse das nichtlineare Randwertproblem $-\Delta u = f(u)$
    in $\Omega=(0,1)\times (0,1)$ mit $u=0$ auf $\partial \Omega$.
    Mit dem Differenzenverfahren\footnote{s. Übungsaufgabe 2)}
    ergibt sich
    \begin{gather*}
      A\vec{u} = h^2 \vec{f}(\vec{u})
    \end{gather*}
    ein System nichtlinearer Gleichungen.
  \end{enumerate}
\end{Bspe}

\subsubsection{Nullstellenbestimmung}\index{Nullstellenbestimmung}
\begin{description}
\item[Gegeben]   $D\subseteq \R^n, f\colon D\to \R^m$ stetig
\item[Gesucht]    $x^{*}\in D $ mit $f(x^{*}) = 0$
\end{description}

\subsubsection{Fixpunktgleichung}\index{Fixpunktiteration}
\begin{description}
\item[Gegeben]   $D\subseteq \R^n, g\colon D\to \R^n$ stetig
\item[Gesucht]      $x^{*}\in D $ mit $g(x^{*}) = x^{*}$
\end{description}
Falls m=n ist dies äquivalent zur Nullstellenbestimmung.

\subsectione{Das Bisektionsverfahren}\index{Bisektionsverfahren}
Sei $f\colon[a,b]\to\R $ stetig und $f(a) \cdot f(b) <0$.
Dann folgt aus dem Zwischenwertsatz die Existenz
mindestens einer Nullstelle $x^{*}\in (a,b)$.
\begin{image}{\copyright}
  \begin{tikzpicture}[>=triangle 45]
    \draw[->] (0,0) -- (10,0);
    \draw[->] (0,-1) -- (0,1);
    \draw[smooth,samples=100,domain=1:6] plot(\x,{1-1/14*((\x-6)^2)});
    
    \foreach \x/\xtext in
    {0.5/a,4.5/{x^{(1)}},2.5/{x^{(2)}},8.5/b}
    \draw(\x,2pt)--(\x,-2pt) node[below] {$\xtext$};
    
    \draw (1.5,-2pt) -- (1.5,2pt);
    \draw (1.3,2pt) node[above] {$x^{(3)}$};
    \draw (2,-2pt) -- (2,2pt) node[above] {$x^{(4)}$};
  \end{tikzpicture}
\end{image}\label{im5.1.2}
Eine Idee zur Annäherung ist, eine Folge von Intervallen
$[a^{(i)}, b^{(i)}]\subset  [a^{(i-1)}, b^{(i-1)}] $,
die eine Nullstelle enthalten zu generieren
und mit $b^{(i)}-a^{(i)} \to 0$.
Definiere hierfür
\begin{gather}
  x^{(i+1)}= \frac{1}{2}(b^{(i)}+a^{(i)})
  \label{V.1.1}
\end{gather}
und
\begin{gather}
  [a^{(i+1)}, b^{(i+1)}] \coloneqq \begin{cases}
    [x^{(i+1)}, b^{(i)}] & \text{für } f(a^{(i)})\cdot f(x^{(i+1)}) > 0 \\
    [a^{(i)}, x^{(i+1)}] & \text{für } f(a^{(i)})\cdot f(x^{(i+1)}) < 0
  \end{cases}
  \label{V.1.2}
\end{gather}
Für jedes $i\geq 1$ gilt somit
\begin{gather*}
  b^{(i)}-a^{(i)} = \frac{1}{2^i}(b-a)
\end{gather*}
und es existiert eine Nullstelle $x^{*}$ in 
$[a^{(i)}, b^{(i)}]\in [a^{(i-1)}, b^{(i-1)}]$ für alle $i$.
Damit folgt
\begin{gather*}
  |x^{(i-1)}-x^{*}| \leq \frac{1}{2}(b^{(i)}-a^{(i)}) 
  =  2^{-(i+1)} (b-a) \to 0
\end{gather*}
Also $\lim_{i\to \infty}x^{(i)} = x^{*}$.


\begin{Kore}
  Das oben angegebene Bisektionsverfahren konvergiert, falls
  $f\colon[a,b]\to\R $ stetig ist und 
  $f(a)\cdot f(b) <0$ gilt.
\end{Kore}

\begin{Beme}~
  \begin{enumerate}[a)]
  \item $x^{(i)} $ wird als Intervallmitte, also unabhängig von $f(x^{(i)})$
    gewählt. die Konvergenzgeschwindigkeit hängt 
    von der Länge des Intervalls $[a,b]$
    und der Lage von $x^{*}$ bezüglich der Intervallhalbierung ab.
    Die Konvergenz kann demnach sehr langsam sein.
  \item Ein Vorteil ist, dass keine Differenzierbarkeitsvoraussetzungen nötig sind.
  \item Das Verfahren ist nicht für $f\colon D\to \R^n$ anwendbar.
  \end{enumerate}
\end{Beme}

\sectione{Fixpunktiteration}\index{Fixpunktiteration}
Gesucht sei ein Fixpunkt $x^{*}\in D\subseteq \R^n$ der stetigen Funktion
$g\colon D\to\R^n$, d.h.
\begin{gather}
  x^{*} = g(x^{*}) \label{V.2.1}
\end{gather}
\textit{Idee:}
Nutze \eqref{V.2.1} zur Iteration, d.h. wähle ein $x^{(0)}\in D$, setze 
\begin{gather}
  x^{(k+1)} = g(x^{(k)})  \quad \text{für } k\in 0, 1, \dotsc
  \label{V.2.2}
\end{gather}
Es bedarf noch der Voraussetzung, dass $x^{(k)}\in D~ \forall k$.
Falls $x^{(k)}$ konvergiert, ist der Grenzwert $x^{*}$ ein Fixpunkt,
denn für stetiges $g$ gilt:
\begin{gather}
  x^{*} = \lim_{k\to \infty}x^{(k+1)} 
  = \lim_{k\to \infty}g(x^{(k)})
  \overset{g \text{ stetig}}{=} g(\lim_{k\to\infty}x^{(k)}) 
  = g(x^{*})
  \label{V.2.3}
\end{gather}


\begin{Bspe}
  Löse $x-e^{-x}-1 = 0$.
  \begin{enumerate}[a)]
  \item $x=1+e^{-x} \eqqcolon g_1(x)$
    \begin{image}{\copyright~ Konvergenz der Fixpunktiteration für $x=1+e^{-x}$}
      \begin{tikzpicture}[>=triangle 45,scale=2]
        \draw[->] (0,0) -- (4.4,0);
        \draw[->] (0,0) -- (0,3);
        \draw[smooth,samples=100,domain=-0.5:3.8] plot(\x,{1+exp(-\x)});
        \draw (3.8,1) node[right] {$g_1(x)$};
        \draw[smooth,samples=100,domain=0:2] plot(\x,{\x});
        \draw[dash pattern=on 3pt off 3pt] (0,1.818730753) node[left] {$x^{(1)}$} -- (0.2,1.818730753) -- (0.2,0) node[below] {$x^{(0)}$};
        \draw[dash pattern=on 3pt off 3pt] (0,1.162231532) node[left] {$x^{(2)}$} -- (1.818730753,1.162231532) -- (1.818730753,0) node[below] {$x^{(1)}$};
        \draw[dash pattern=on 3pt off 3pt] (0,1.312787406) node[left] {$x^{(3)}$} -- (1.162231532,1.312787406) -- (1.162231532,0) node[below] {$x^{(2)}$};
      \end{tikzpicture}
    \end{image}
    \label{im5.2.1(1)}
    $\longrightarrow$ Konvergenz

  \item $e^{-x} = x-1   \Leftrightarrow x= -ln(x-1) \eqqcolon g_2(x)$
    \begin{image}{\copyright Versagen der Fixpunktiteration für $x=-ln(x-1)$}
      \begin{tikzpicture}[>=triangle 45,scale=2,x=1.5cm,y=0.7cm]
        \draw[->] (0,0) -- (3,0);
        \draw[->] (0,0) -- (0,3);
        \draw[smooth,samples=100,domain=1.07:2.8] plot(\x,{-ln(\x-1)});
        \draw[smooth,samples=100,domain=0:2] plot(\x,{\x});
        
        \draw(1,2pt) -- (1,-2pt) node[below left] {1};
        \draw(2,2pt) -- (2,-2pt) node[below] {2};
        \draw[dash pattern=on 3pt off 3pt] (1.2,0) node[below] {$x^{(0)}$} -- (1.2,1.609437912);
        \draw[dash pattern=on 3pt off 3pt] (1.609437912,0) node[below] {$x^{(1)}$} -- (1.609437912,1.609437912);
      \end{tikzpicture}
    \end{image}
    \label{im5.2.1(2)}
    $\longrightarrow$ $g(x^{(2)}) $ nicht definiert!
  \end{enumerate}
\end{Bspe}

\begin{Defe}
  \index{Kontraktion}
  Sei $D\subseteq  \R^n $ abgeschlossen und $\nn$ eine Norm auf dem $\R^n$.
  Eine Abbildung $g\colon D\to \R^n $ heißt 
  \textbf{Kontraktion}\index{Kontraktion} bezüglich  $\nn$,
  falls es ein $\kappa \in [0,1)$ gibt mit
  \begin{gather*}
    \nn[g(u)-g(v)] \leq \kappa \nn[u-v] \quad \forall u,v\in D
  \end{gather*}
  Die kleinste solche Zahl $\kappa$ heißt Kontraktionszahl von $g$.
  \begin{image}{\copyright~ Veranschaulichung von Kontraktion: $g_1$
      Kontraktion, $g_2$ nicht}
    \begin{tikzpicture}
      \draw[->] (0,0) -- (0,4) node[anchor=east] {$g_1(x)$};
      \draw[->] (0,0) -- (4,0) node[anchor=north] {x};
      \foreach \x/\xtxt in {0.5/{$u$}, 2.5/{$v$}}
      {
        \draw (\x,0.1)--(\x,-0.1) node[anchor=north] {\xtxt};
      }
      \foreach \y/\ytxt in {0.5/{$u$}, 2.5/{$v$},1.0/{$g_1(u)$},2.0/{$g_1(v)$}}
      {
        \draw (0.1,\y)--(-0.1,\y) node[anchor=east] {\ytxt};
      }
      \draw[dashed, line width=1pt] (0.5,0.5) rectangle (2.5,2.5);
      \draw (0.5,1) rectangle (2.5,2);
    \end{tikzpicture}
    \begin{tikzpicture}
      \draw[->] (0,0) -- (0,4) node[anchor=east] {$g_2(x)$};
      \draw[->] (0,0) -- (4,0) node[anchor=north] {x};
      \foreach \x/\xtxt in {0.5/{$u$}, 2.5/{$v$}}
      {
        \draw (\x,0.1)--(\x,-0.1) node[anchor=north] {\xtxt};
      }
      \foreach \y/\ytxt in {0.5/{$u$}, 2.5/{$v$},0.2/{$g_2(u)$},3.5/{$g_2(v)$}}
      {
        \draw (0.1,\y)--(-0.1,\y) node[anchor=east] {\ytxt};
      }
      \draw[dashed,line width=1pt] (0.5,0.5) rectangle (2.5,2.5);
      \draw (0.5,0.2) rectangle (2.5,3.5);
    \end{tikzpicture}
    % \begin{tikzpicture}
    %   \draw (0,4) node[below right] {$u$} -- (3,4) node[below left] {$v$} node[right] {$g(v)$} -- (3,5) node[right] {$g(u)$}-- (0,5) -- cycle;
    
    %   \draw (0,0) node[below] {$u$} -- (1,0) node[below left] {$v$} node[above right] {$g(u)$} -- (1,3) node[below right] {$g(v)$} -- (0,3)  -- cycle;
    
    %   \draw (6.5,4.5) node[left] {Kontraktion};
    %   \draw (6.5,1.5) node[left] {keine Kontraktion};
    % \end{tikzpicture}
  \end{image}
  \label{im5.2.2}
  Offensichtlich ist jede auf $D$ kontrahierende Abbildung stetig.
\end{Defe}  


\begin{Leme}
  \label{5.2.3}
  Sei $D=\overline{\Omega} $ mit $\Omega \subseteq \R^n$ offen und konvex
  und $\nn$ eine Norm auf dem $R^n$.
  Falls $g\colon D\to D$ eine stetig differenzierbare Funktion ist und
  bezüglich der zugeordneten Matrixnorm $\sup_{x\in \Omega}\nn[Dg(x)]<1$ gelte,
  so ist $g$ kontrahierend bezüglich $\nn$.
\end{Leme} 

\begin{proof}
  Mit $u,v \in D$ gilt $u+t(v-u)\in D$, da $D$ konvex ist.
  Somit ist $h\colon [0,1]\to \R^n$ 
  mit $h(t) \coloneqq g(u+t(v-u))$ wohldefiniert
  und stetig differenzierbar.
  Mit dem Hauptsatz der Differenzial- und Integralrechnung folgt:
  \begin{align}\nonumber
    \nn[g(u)-g(v)] 
    & = \nn[h(1)-h(0)]  \\ \nonumber
    & = \nn[\int_{0}^{1} h'(t) dt] \\ \nonumber
    & = \nn[\int_{0}^{1} Dg(u+t(v-u))\cdot (v-u)dt] \\ \nonumber
    & \leq \int_{0}^{1} \nn[Dg(u+t(v-u))]dt \cdot \nn[v-u] \\
    & \leq \underbrace{\sup_{x\in\Omega}\nn[Dg(x)]}_{\eqqcolon \kappa} 
      \cdot \nn[v-u]
      \label{V.2.4}
  \end{align}
\end{proof}

\begin{Satze}[Banachscher Fixpunktsatz]
  \index{Banachscher Fixpunktsatz}
  \label{5.2.4}
  Sei $D\subset \R^n$ abgeschlossen und die Abbildung 
  $g\colon D\to \R^n$  eine Kontraktion.
  Dann gilt:
  \begin{enumerate}[1)]
  \item Es existiert genau ein Fixpunkt $x^{*}$ von $g$.
  \item Für jeden Startwert $x^{(0)}\in D$ konvergiert die 
    Folge der Fixpunktiterierten
    \begin{gather}
      x^{(k+1)} \coloneqq g(x^{(k)})  ~
      \overset{\mathclap{k\rightarrow \infty}}{\longrightarrow}~ x^{*}
      \label{V.2.5}
    \end{gather}
  \item Es gelte die a posteriori Fehlerabschätzung
    \begin{gather}
      \nn[x^{(k)}-x^{*}] \leq \frac{\kappa}{1-\kappa} \nn[x^{(k)}-x^{(k-1)}]
      \label{V.2.6}
    \end{gather}
    und die a priori Fehlerabschätzung
    \begin{gather}
      \nn[x^{(k)}-x^{*}] \leq \frac{\kappa^k}{1-\kappa} \nn[x^{(1)}-x^{(0)} ]
      \label{V.2.7}
    \end{gather}
  \end{enumerate}
\end{Satze}

% -----------------------------------------------------------------------------

\begin{proof}
  \marginpar{19.11.2014}
  \begin{description}
  \item[zu 2)] Sei $x_0\in D$ beliebig. 
    \eqref{V.2.5} ist wohldefiniert, da $g(D)\subset D$.
    $(x^{(k)})_{k\in\N}$ bildet eine Cauchyfolge, da 
    \begin{align*}
      \nn[x^{(k+1)}-x^{(k)}] 
      &= \nn[g(x^{(k)})-g(x^{(k-1)})] \\
      &\leq \kappa \nn[x^{(k)}-x^{(k-1)}] \\
      &\leq \kappa^k\nn[x^{(1)}-x^{(0)}] \\
      \Longrightarrow~~ \nn[x^{(k+l)}-x^{(k)}] 
      &\leq \sum_{i=0}^{l}\nn[x^{(k+i+1)}-x^{(k+i)}] \\
      &\leq \sum_{i=0}^{l}\kappa^{k+1}\nn[x^{(1)}-x^{(0)}]\\
      &\leq \frac{\kappa^k}{1-\kappa}\nn[x^{(1)}-x^{(0)}]
      &&\forall l\in\N 
    \end{align*}
    Daraus folgt, dass 
    $\lim\limits_{k\rightarrow \infty} x^{(k)}=x^{*} $ existiert 
    und $x^{*}\in D$, da $D$ abgeschlossen ist und somit vollständig.

  \item[zu 1)] Da $g$ stetig ist, 
    ist $g(x^{*})=x^{*}$ (siehe hierzu \eqref{V.2.3}). 
    $x^{*}$ ist eindeutiger Fixpunkt, 
    da für einen weiteren Fixpunkt $y^{*}$ gilt
    \begin{gather*}
      0\leq\nn[x^{*}-y^{*}]=\nn[g(x^{*})-g(y^{*})]\leq\kappa\nn[x^{*}-y^{*}]
    \end{gather*}
    Da $\kappa<1$, muss $ \nn[x^{*}-y^{*}]=0$ sein und damit $x^{*}=y^{*}$.
    
  \item[zu 3)] Betrachte 
    \begin{gather*}
      \nn[x^{*}-x^{(k)}] 
      = \lim\limits_{l\rightarrow \infty} \nn[x^{(k+l)}-x^{(k)}]
      \leq  \frac{\kappa^k}{1-\kappa} \nn[x^{(1)}- x^{(0)}]
    \end{gather*}
    bzw.
    \begin{align*}
      \lim\limits_{l\rightarrow \infty} \nn[x^{(k+l)}-x^{(k)}]
      &\leq \lim\limits_{l\rightarrow \infty} \sum_{i=0}^{l-1}\nn[x^{(k+i+1)}-x^{(k+i)}] \\
      &\leq \lim\limits_{l\rightarrow \infty}\sum_{i=0}^{l-1}\kappa^{i+1} \nn[x^{(k)}-x^{(k-1)}] \\
      &\leq \frac{\kappa}{1-\kappa} \nn[x^{(k)}-x^{(k-1)}] 
    \end{align*}
  \end{description}
\end{proof}

\begin{Beme}
  \label{5.2.5}~
  \begin{enumerate}[1)]
  \item Als Voraussetzung wäre bereits ausreichend:
    $D$ ist vollständiger metrischer Raum mit Metrik $d$.
    Dann ersetze die Norm durch die Metrik $d$.
  \item Im Allgemeinen ist der Nachweis $g(D)\subset D$ schwierig.
  \end{enumerate}
\end{Beme}

\begin{Fole}
  \label{5.2.6}
  Sei $x^{*}\in \R^n$, so dass $g(x^{*})=x^{*}$, 
  und sei $g$ in einer Umgebung von 
  $\overline{B_\varepsilon(x^{*})}
  =\left\{ x\in \R^n \middle\vert \nn[x-x^{*}]\leq \varepsilon \right\}$
  stetig differenzierbar und es gelte 
  $\nn[g'(x)]<1$ für $x\in \overline{B_\varepsilon(x^{*})}$,
  so ist Satz \ref{5.2.4} mit $D=\overline{B_\varepsilon(x^{*})}$ anwendbar.
\end{Fole}

\begin{proof}
  Nutze \eqref{V.2.4} und Lemma \ref{5.2.3}.
\end{proof}


\sectione{Konvergenzordnung und Fehlerabschätzungen}

\begin{Defe}
  \label{5.3.1}
  Eine Folge $(x^{(k)})_{k\in\N} $ mit $x^{(k)}\in\R^n$ \textbf{konvergiert} mit (mindestens)
  der \textbf{Ordnung}\index{Konvergenz!Ordnung} 
  $p\geq 1$ gegen $x^{*}$, falls $\lim_{k\to\infty}x^{(k)}=x^{*}$
  und falls es ein $C>0$ sowie $N\in\N$ gibt, so dass
  \begin{gather*}
    \nn[x^{(k+1)}-x^{*}] \leq C \nn[x^{(k)}-x^{*}]^p\qquad \forall k\geq N 
  \end{gather*}
  Im Fall $p=1$ ist zusätzlich $C<1$ und man spricht von \textbf{linearer Konvergenz}\index{Konvergenz!linear}.
  Für $p=2$ heißt es 
  \textbf{quadratische Konvergenz}\index{Konvergenz!quadratisch}. 
  Gilt 
  \begin{gather*} 
    \lim_{k\to\infty}\frac{\nn[x^{(k+1)}-x^{*}]}{\nn[x^{(k)}-x^{*}]}=0\, ,
  \end{gather*} so konvergiert die Folge \textbf{superlinear}\index{Konvergenz!superlinear}.
\end{Defe}

\begin{Beme}
  Die Fixpunktiteration konvergiert unter der Voraussetzung in \ref{5.2.4} mindestens linear.
\end{Beme}

\begin{Beme}~
  \begin{enumerate}[a)]
  \item  lineare Konvergenz hängt von der gewählten Norm ab.
  \item Hat die Folge bzgl. einer Vektornorm 
    auf dem $\R^n$ die Konvergenzordnung $p>1$,
    hat sie diese bzgl. jeder Norm.
  \end{enumerate}
\end{Beme}

\begin{Defe}~
  \label{5.3.4}
  \begin{enumerate}[a)]
  \item Ein iteratives Verfahren zur Bestimmung eines Wertes $x^{*}$ hat 
    die Konvergenzordnung $p$, falls es eine Umgebung $U$ um $x^{*}$ gibt, 
    so dass für alle Startwerte aus $U\backslash \{x^{*}\}$
    die erzeugte Folge mit Ordnung $p$ konvergiert.
  \item Das Verfahren heißt \textbf{lokal konvergent}\index{Konvergenz!lokal},
    falls es für alle Startwerte in einer Umbegung von $x^{*}$ konvergiert.
  \item Das Verfahren heißt \textbf{global konvergent}\index{Konvergenz!global},
    falls es im gesamten Definitionsbereich des zugehörigen Problems konvergiert.
  \end{enumerate}
\end{Defe} 

\begin{Leme}
  \label{5.3.5}
  Sei $(x^{(k)})_{k\in\N}$ eine konvergente Folge in $\R$ mit Grenzwert $x^{*}$.
  \begin{enumerate}[a)]
  \item Falls 
    \begin{gather}
      \lim_{k\to \infty}\frac{x^{(k+1)}-x^{*}}{x^{(k)}-x^{*}}
      = A\in (-1,1)\, ,~ A\neq 0
      \label{V.3.1}
    \end{gather}
    hat die Folge genau die Konvergenzordnung 1.
    Weiter gilt mit $A_k=\frac{x^{(k)}-x^{(k-1)}}{x^{(k-1)}-x^{(k-2)}}$
    \begin{align}
      \lim_{k\to \infty}\frac{A_k}{1-A_k}\cdot 
      \frac{x^{(k)}-x^{(k-1)}}{x^{*}-x^{(k)}}&=1 
      &\text{ sowie }&
      &\lim_{k\to\infty}A_k=A
        \label{V.3.2}
    \end{align}
  \item Falls die Folge Konvergenzordnung $p>1$ hat, gilt
    \begin{gather}
      \lim_{k\to\infty}\frac{x^{(k)}-x^{(k-1)}}{x^{*}-x^{(k)}}=1
      \label{V.3.3}
    \end{gather}
  \item[\textbf{zu}]\textbf{Def.} \ref{5.3.1}:  
    Im Fall $p=1$ ist zusätzlich $C<1 $ verlangt.
  \end{enumerate}
\end{Leme} 

\begin{proof}
  \textit{(skizzenhaft, siehe Übungsaufgaben)}
  Sei $e^{(k)}\coloneqq x^{*}-x^{(k)}$.
  Nutze $x^{(k+1)}-x^{(k)} = e^{(k)}-e^{(k+1)}$:
  \begin{enumerate}[a)]
  \item Zeige
    $\lim_{k\to\infty}\frac{x^{(k)}-x^{(k-1)}}{e^{(k)}} = \frac{1-A}{A}$,
    sowie $\lim_{k\to \infty}A_k = A$ und es folgt die Behauptung.
  \item Folgt aus $\lim_{k\to\infty} \frac{e^{(k+1)}}{e^{(k)}} =0$.
  \end{enumerate}
\end{proof}

\begin{Fole}[a posteriori Fehlerabschätzung]~
  \begin{enumerate}[a)]
  \item Für $p=1$, große $k$ und $A_k$ in etwa konstant gilt
    \begin{gather}
      x^{*}-x^{(k)} \approx \frac{A_k}{1-A_k}(x^{(k)}-x^{(k-1)})
      \label{V.3.4}
    \end{gather}
    $|x^{(k)}-x^{(k-1)}|$ ist im Allgemeinen \textbf{keine} sinnvolle Schätzung
    des Fehlers $|x^{*}-x^{(k)}|$!
  \item Für $p>1$ und für große $k$ gilt
    \begin{gather}
      x^{*}-x^{(k)} \approx x^{(k+1)}-x^{(k)}
      \label{V.3.5}
    \end{gather}
  \end{enumerate}
\end{Fole}


\begin{Beme}
  Für Folgen im $\R^n$ gibt es für $p=1$ kein Analogon zu \eqref{V.3.4}.
  Falls $p>1$, lässt sich \eqref{V.3.3} für die Normen der Differenzen zeigen,
  d.h.
  \begin{gather}
    \nn[x^{*}-x^{(k)}] \approx \nn[x^{(k+1)}-x^{(k)}]
    \label{V.3.6}
  \end{gather}
  
  \begin{proof}
    Nutze $\lim_{k\to\infty} \frac{\nn[e^{(k+1)}]}{\nn[e^{(k)}]} =0$ und 
    \begin{gather*}
      \nn[e^{(k)}]-\nn[e^{(k+1)}]
      \leq \nn[x^{(k+1)}-x^{(k)}] 
      \leq \nn[e^{(k)}]+\nn[e^{(k+1)}] \, .
    \end{gather*}
  \end{proof}
\end{Beme}


\begin{Fole}\index{Konvergenz!Ordnung}
  Falls $p>1$ ist, kann $p$ folgendermaßen approximiert werden:
  \begin{gather*}
    p \approx \frac{\log(\nn[x^{(k+2)}-x^{(k+1)}])}
    {\log(\nn[x^{(k+1)}-x^{(k)}])}
  \end{gather*}
\end{Fole}

\begin{proof}
  Siehe Übungsaufgabe.
\end{proof}

\sectione{Newton-Verfahren für skalare Gleichung}\index{Newton-Verfahren}
Sei $f\colon[a,b]\to\R$ differenzierbar. Dann gilt
\begin{gather*}
  f(x^{*}) = f(x) + f'(x)(x^{*}-x)+o(\nn[x-x^{*}]) \, ,
\end{gather*} 
d.h. $f$ kann lokal gut durch die Tangente approximiert werden.

Betrachte für das Newton-Verfahren nun 
die Nullstellengleichung $f(x^{*}) = 0$
und bestimme iterativ die Nullstelle der Tangentengleichung
\begin{gather*}
  0=f(x) + f'(x)(\overline{x}-x) 
  \Leftrightarrow \overline{x}= x-\frac{f(x)}{f'(x)}
\end{gather*}
Notwendig ist hier die Bedingung $f'(x) \neq 0$.
% Newton-Verfahren Gleichung
\begin{image}{\copyright Newton-Verfahren bei einem Funktionsgraphen}
  \begin{tikzpicture}[>=triangle 45,scale=2]
    \draw[->] (0,0) -- (5,0);
    \draw[->] (0,0) -- (0,4.5);
    \draw[smooth,samples=100,domain=0.5:3.8] plot(\x,{0.8*\x^3-24/5*\x^2+36/5*\x +0.3});
    \draw[smooth,samples=100,domain=0.7:3.461979167]
    plot(\x,{-1.536*\x +5.3176});
    
    \foreach \x/\xtext in
    {0.5/a,1.4/{x^{(0)}},3.461979167/{x^{(1)}},3.8/b}
    \draw(\x,2pt)--(\x,-2pt) node[below] {$\xtext$};
  \end{tikzpicture}
\end{image}
\label{im5.4}


\subsectione{Iterationsschritt des Newton(-Kantorowitsch)-Verfahrens}
\begin{gather}
  x^{(k+1)} = x^{(k)} - \frac{f(x^{(k)})}{f'(x^{(k)})}
  \label{V.4.1}
\end{gather}
Das Newton-Verfahren wird auch 
Tangentenverfahren\index{Tangentenverfahren} genannt 
und stammt von J. Raphson (1630).
Newton hat eine ähnliche Technik früher angewendet.

\begin{Satze}
  \label{5.4.2}
  Sei $f\in C^1(a,b)$ und $x^{*}\in (a,b)$ 
  eine einfache Nullstelle von $f$, d.h. $f'(x^{*})\neq 0$.
  Dann gibt es ein  $\varepsilon >0$, 
  s.d. für jedes $x^{(0)}\in\overline{B_\varepsilon(x^{*})}$
  das Newton-Verfahren \eqref{V.4.1} superlinear gegen $x^{*}$ konvergiert.
  Falls $f\in C^2(a,b) $ tritt mindestens quadratische Konvergenz ein,
  d.h. das Verfahren konvergiert lokal quadratisch.
\end{Satze}

\begin{proof}
  Gleichung \eqref{V.4.1} definiert eine Fixpunktiteration mit $g(x) = x-\frac{f(x)}{f'(x)}$.
  Für $f\in C^2(a,b)$ gilt 
  \begin{gather*}
    g'(x) = 1- \frac{f'f'-ff''}{(f')^2}(x)
    = \frac{f(x)f''(x)}{(f'(x))^2}\, .
  \end{gather*}
  Da $f(x^{*})= 0$ und $f'(x^{*})\neq 0$ gilt $g'(x^{*})=0$ .
  Weiterhin gibt es eine Umgebung $U_0$ von $x^{*}$, 
  in der $f(x)\neq 0~\forall x\in U_0$, da $f$ stetig ist.
  In $U_0$ ist somit $g' $ stetig. 
  Da $g'(x^{*})=0$ ist, existiert ein $\varepsilon>0$ mit
  \begin{gather*}
    g'(x)\leq \kappa<1 \quad \forall x\in \overline{B_\varepsilon(x^{*})}\, .
  \end{gather*}
  Da $g(x^{*})=x^{*}$ ist, ist die Folgerung \ref{5.2.6} anwendbar,
  also ist $g$ eine Kontraktion und 
  $g(\overline{B_\varepsilon(x^{*})})\subset\overline{B_\varepsilon(x^{*})}$.
  Der Banachsche Fixpunktsatz liefert Konvergenz 
  für alle $x^{(0)}\in\overline{B_\varepsilon(x^{*})}$.
  
  Die quadratische Konvergenz folgt aus 
  \begin{align*}
    \left|x^{(k+1)}-x^{*}\right| 
    &= \left| 
      x^{(k)}
      -\frac{f\left(x^{(k)}\right)}{f'\left(x^{(k)}\right)}
      -x^{*}
      +\frac{f\left(x^{*}\right)}{f'\left(x^{*}\right)}
      \right| \\
    &= \frac{\left| 
      f\left(x^{(*)}\right)
      -f\left(x^{(k)}\right)
      +f'\left(x^{(k)}\right)\left(x^{(k)}-x^{*}\right)
      \right|}
      {\left|f'\left(x^{(k)}\right)\right|}\\
    &\leq
      \sup_{x\in\overline{B_\varepsilon\left(x^{*}\right)}}
      \frac{1}{\left|f'\left(x\right)\right|}
      \cdot
      \sup_{x\in\overline{B_\varepsilon(x^{*})}}
      \left|f''(x)\right|\cdot\frac{1}{2}\left|x^{(k)}-x^{*}\right|^2
  \end{align*}
  aufgrund der Taylorentwicklung und da
  $x^{(k)}\in\overline{B_\varepsilon(x^{*})}
  ~\forall k\in\N$ (da $g$ Kontraktion).
  
  Für $f\in C^1$ siehe \cite{haemmerlinhoffmann}.
\end{proof}

\begin{Beme}~
  \begin{enumerate}[a)]
  \item Mehrfache Nullstellen können im Allgemeinen
    nicht mit \eqref{V.4.1} bestimmt werden.
  \item Die Ableitung $f'$ muss analytisch (als Funktion) gegeben sein.
  \item Die Lage und Größe des Konvergenzintervalls ist a priori unbekannt.\\
    (Hierfür könnte z.B. das Bisektionsverfahren Anwendung finden.)
  \end{enumerate}
\end{Beme}



% ----------------------------------------------------------------

% \subsectione{Beispiele: Newton-Verfahren ohne Konvergenz}
\begin{Bspe}[Newton-Verfahren ohne Konvergenz]~
  \marginpar{24.11.2014}
  \begin{itemize}
  \item $x^{(1)}$ nicht mehr im Definitionsbereich\\
    \begin{image}{\copyright Fehlschlagen des Newton-Verfahrens: außerhalb des
        Definitionsbereichs}
      \begin{tikzpicture}[>=triangle 45,scale=2]
        \draw[->] (-2,0) -- (5,0) node [right] {$x$};
        \draw[->] (0,0) -- (0,2) node[left] {$y$};
        \draw[smooth,samples=100,domain=0.55:5] plot(\x,{ln(\x)});
        \draw[smooth,samples=100,domain=-2:5]
        plot(\x,{1/3*\x+0.09861});
        
        \draw (3,1.18) -- (3,1.02);
        
        \foreach \x/\xtext in
        {1/x^*,3/x^{(0)}, -0.295836866/x^{(1)}}
        \draw(\x,2pt)--(\x,-2pt) node[below] {$\xtext$};
      \end{tikzpicture}
    \end{image}
    \label{im5.4.4(1)}

  \item $|x^{*}-x^{(1)}| \nless |x^{*}-x^{(0)}| $ ist keine
    Kontraktion, da die Konvergenz nicht gesichert ist.

    % 5.4.4 keine Kontraktion
    % PROBLEM: Graph muss Pol in der Gegend des Geradenschnitts haben
    % \begin{image}{\copyright Fehlschlagen des Newton-Verfahren: Konvergenz nicht
    %   gesichert}
    %   \begin{tikzpicture}[>=triangle 45,scale=2]
    %     \draw[->] (0,0) -- (4,0) node [right] {$x$};
    %     \draw[->] (0,0) -- (0,2) node[left] {$y$};
    %     \draw[smooth,samples=100,domain=0.7:3.3] plot(\x,{-(\x-2)^2+1});
    %     \draw[smooth,samples=100,domain=1.7:3.3]
    %     plot(\x,{-1.2*\x+3.76});
    
    %     \draw (2.6,0.51) -- (2.6,0.77);
    
    %     \foreach \x/\xtext in
    %     {2.6/x^{(0)},3.1333/x^{(1)}}
    %     \draw(\x,2pt)--(\x,-2pt) node[below] {$\xtext$};
    %   \end{tikzpicture}\\
    % \end{image}
    % \label{im5.4.4(2)}
  \end{itemize}
\end{Bspe}

\subsectione{Newton-Verfahren: Iterativer Linearisierungsprozess}
Die entscheidende Idee beim Newton-Verfahren ist 
der \textbf{iterative Linearisierungsprozess}
\index{iterativer Linearisierungsprozess}, 
d.h. die Lösung einer nichtlinearen Gleichung wird
durch eine Folge von Lösungen linearer Gelichungen ersetzt.

\begin{Bspe}
  \label{5.4.6}
  Es ist die Lösung von $x-e^{-\frac{1}{2}x}=0$ mit $x^{(0)}=0,8$ gesucht.
  \begin{enumerate}[a)]
  \item Mit der Banachschen Fixpunktiteration angewendet auf 
    $x=e^{-\frac{1}{2}x}$ ergibt sich
    \begin{gather*}
      x^{(10)}=\text{\textbf{0,7034}}7017 
      \qquad \text{auf 4 Stellen exakt}
    \end{gather*}
  \item Mit dem Newton-Verfahren
    \begin{align*}
      x^{(3)}&= 0,70346742 &&\text{bis auf 17 Stellen exakt}\\
      x^{(4)} &&& \text{bis auf Maschinengenauigkeit exakt}
    \end{align*}		
  \end{enumerate}
\end{Bspe}
Die Ableitung $f'(x)$ ist nicht immer explizit bekannt.
Eine Idee ist, sie zu approximieren mithilfe des Differenzenquotienten:
\begin{gather*}
  f'(x^{(k)})  \approx \frac{f(x^{(k)})-f(x^{(k-1)})}{x^{(k)}-x^{(k-1)}}
\end{gather*}
Damit ergibt sich
\begin{gather*}
  x^{(k+1)} = x^{(k)}-f(x^{(k)}) \frac{x^{(k)} 
    - x^{(k-1)}}{f(x^{(k)})-f(x^{(k-1)})}
\end{gather*}\index{Sekantenverfahren}
d.h. $x^{(k+1)} $ ist die Nullstelle der Sekante 
durch $f(x^{k})$ und $f(x^{(k-1)})$.

\subsectione{Iterationsschritt des Sekantenverfahrens}
\begin{gather}
  x^{(k+1)} = \frac{x^{(k-1)}f(x^{(k)}) 
    - x^{(k)}f(x^{(k-1)})}{f(x^{(k)})-f(x^{(k-1)})}
  \label{V.4.2}
\end{gather}


\begin{image}{\copyright Geometrische Veranschaulichung Sekantenverfahren}
  \begin{tikzpicture}[>=triangle 45,scale=2,x=1cm,y=2cm]
    \draw[->] (0,0) -- (3.7,0) node [right] {$x$};
    \draw[->] (0,0) -- (0,1.3) node[left] {$y$};
    \draw[smooth,samples=100,domain=0.2:3.6] plot(\x,{0.1*\x^2-0.2});
    
    % Punkte zeichnen
    \foreach \x/\y in
    {0.3/-0.191,3.4/0.956,0.816/-0.1334144,1.132/-0.0718576}
    \filldraw (\x,\y) circle (0.02cm);
    
    % Sekanten zeichnen
    \draw (0.3,-0.191) -- (3.4,0.956) -- (0.816,-0.1334144) -- (1.132,-0.0718576);
    
    \foreach \x/\xtext in
    {0.3/x^{(k-1)},3.4/x^{(k)},0.816/x^{(k+1)}}
    \draw(\x,-2pt)--(\x,2pt) node[above] {$\xtext$};
  \end{tikzpicture}
\end{image}
\label{im5.4.7}

\begin{Satze}[Konvergenz des Sekantenverfahrens]
  Sei $f\in C^2([a,b])$ und $x^{*}\in (a,b)$ eine einfache Nullstelle.
  Dann konvergiert das Sekantenverfahren in einer Umbegung von $x^{*}$
  superlinear mit Ordnung 
  \begin{gather*}
    p=\frac{1}{2}(1+\sqrt{5})= 1,618 \, .
  \end{gather*}
\end{Satze}

\begin{proof}
  Siehe z.B. \cite[][Zwischenwertsatz, Fibonacci-Folge]{haemmerlinhoffmann,stoerbulirsch}
\end{proof}

\textbf{zu Beispiel} \ref{5.4.6}: Das Sekantenverfahren benötigt
einen zweiten Startwert, z.B.
\begin{align*}
  x^{(1)}&=0,7 \\
  \Rightarrow ~ x^{(3)} &= 0,7034674 
                        &&\text{auf 7 Stellen exakt}\\
  x^{(6)} &&& \text{bis auf Maschinengenauigkeit exakt}
\end{align*}


\begin{Beme}~
  \begin{enumerate}[a)]
  \item Das Verfahren ist keine Fixpunktiteration.
    Es benötigt $x^{(k)}$ und $x^{(k-1)}$ für $x^{(k+1)}$
    (\textbf{Mehrschrittverfahren})\index{Mehrschrittverfahren}
  \item Die Berechnung von $f(x)$ und $f'(x)$ ist im Allgemeinen
    sehr teuer. Das Sekanten-Verfahren benötigt pro Iteration
    nur eine Funktionsauswertung, das Newton-Verfahren hingegen zwei.
    Also sind zwei Iterationen des Sekanten-Verfahrens so teuer wie eine
    des Newton-Verfahrens. 
    Bei gleichem Aufwand konvergiert das Sekanten-Verfahren daher lokal
    schneller als das Newton-Verfahren mit der Konvergenzordnung 
    $p^2= 2,618\dotsc$ für $x^{(k)}\to x^{(k+2)}$ 
    (siehe auch Beispiel \ref{5.4.6}).
    
    \textit{Beispiel:} 
    Sei $f:\R^n\rightarrow\R$ und $f(x)$ die erste Komponente von $ A^{-1}x$.
    Diese n-dimensionale Funktionsauswertung benötigt $\mathcal{O}(n^3)$ flops.
  \item Die Sekantenmethode ist i.A. nicht stabil, 
    denn für $f(x^{(k)})\approx f(x^{(k+1)})$
    können Stellenauslöschungen im Nenner auftreten.
    Stabilere Varianten, wie z.B. \textbf{regula falsi}, 
    haben eine geringere Konvergenzordnung.
  \end{enumerate}
\end{Beme}



\sectione{Das Newton-Verfahren im Mehrdimensionalen} 
\index{Newton-Verfahren!mehrdimensional}
Wie im 1-dimensionalen wird $f\colon\Omega\subseteq\R^n\to\R^n$
linearisiert 
\begin{gather}
  f(\overline{x}) \approx f(x) +\D f(x)(\overline{x}-x)
  \label{V.5.1}
\end{gather}
mit
\begin{gather*}
  \D f(x) = \begin{pmatrix}
    \frac{\partial f_1}{\partial x_1}(x) 
    &\dots & \frac{\partial f_1}{\partial x_n}(x)\\
    \vdots && \vdots\\
    \frac{\partial f_n}{\partial x_1}(x) 
    &\dots & \frac{\partial f_n}{\partial x_n}(x)
  \end{pmatrix}
  \qquad \text{(genannt: die Jacobi-Matrix von f)}
\end{gather*}
Falls nun die Jacobi-Matrix $\D f(x)$ invertierbar
ist und $f(\overline{x})= 0$ gilt, folgt
\begin{gather*}
  \overline{x} = x-[\D f(x)]^{-1}\cdot f(x)
\end{gather*}

\subsectione{Iterationsschritt des Newton-Verfahrens}
\begin{gather}
  x^{(k+1)} = x^{(k)} -[\D f(x^{(k)})]^{-1}\cdot f(x^{(k)})
  \label{V.5.2}
\end{gather}

\subsectione{Newton-Verfahren}
\begin{pseudocode}{0.5\linewidth}
  setze Startwert $x$ \\
  $i=0$ \\
  $fx= f(x)$ \\
  \textbf{while} \enquote{Abbruchkriterium} \\
  |	\> $\D fx = \D f(x)$ \\
  |	\> Löse\footnotemark $\D fx\cdot d=-fx$ \\
  |	\> $x=x+d$ \\
  |	\> $fx=f(x)$\\
  |	\> $i=i+1$\\
  \textbf{end}
\end{pseudocode}
\footnotetext{entspricht $Ax=b$}

% \subsectione{Bemerkung}
\begin{Beme}
  Ein Newton-Iterationsschritt \eqref{V.5.2} wird also 
  aufgeteilt in Berechnung der sogenannten 
  \textbf{Newton-Korrektur}\index{Newton-Verfahren!Newton-Korrektur}
  \begin{gather}
    \D f(x^{(k)})\Delta x^{(k)} = -f(x^{(k)}) \label{V.5.3}
  \end{gather}
  und dem \textbf{Korrekturschritt}\index{Newton-Verfahren!Korrekturschritt}
  \begin{gather}
    x^{(k+1)}= x^{(k)}+\Delta x^{(k)} \label{V.5.4}
  \end{gather}
\end{Beme}


\subsectione{Aufwand pro Iteration}
\begin{itemize}
\item[\textbf{$n$}] eindimensionale Funtionsauswertungen für $f(x)$
\item[\textbf{$n^2$}] eindimensionale Funtionsauswertungen für $\D f(x)$
\item[$\mathcal{O}(n^2)$] flops (i.d.R.) zum Lösen eines GLS
\end{itemize}

% ---------------------------------------------------------------------

% \subsectione{Bemerkung}
\begin{Beme}
  \label{5.5.5}
  \marginpar{26.11.2014}
  Das Newton-Verfahren ist \textbf{affin-invariant}\index{affin-invariant},
  d.h. die Folge $(x^{(k)})$ ist zu gegebenem $x^{(0)}$ unabhängig davon,
  ob $f(x)=0$ oder $\widetilde{f}(x)\coloneqq A\cdot f(x) =0$
  mit regulärem $A\in \Renn $ gelöst wird.
  Dies gilt, da 
  \begin{align*}
    [D\widetilde{f}(x)]^{-1} \cdot \widetilde{f}(x)
    &= [A\cdot \D f(x)]^{-1} \cdot (A\cdot f(x))\\
    &= [\D f(x)]^{-1} \cdot f(x)
  \end{align*}
  und damit ist die Newton-Korrektur $\Delta x^{(k)}$ affin-invariant.
\end{Beme}

% \subsectione{Satz}
\begin{Satze}
  Sei $\Omega\in\R^n$ offen und $f\colon\Omega\to\Ren$ in $C^2(\Omega)$.
  Sei $x^{*}\in\Omega $ eine Nullstelle $f$ 
  mit einer invertierbaren Jacobi-Matrix $\D f(x^{*})$.
  Dann existiert eine Umgebung von $x^{*}$, so dass das Newton-Verfahren 
  für jeden Startwert $x^{(0)}$ in dieser Umgebung
  quadratisch gegen $x^{*}$ konvergiert.
\end{Satze}

\begin{proof}
  Der Beweis kann wie im eindimensionalen durchgeführt werden.
  Aber Vorsicht: $D^2f(x)$ ist eine \textbf{bilineare Abbildung} in 
  $\mathcal{L}(\Ren, \mathcal{L}(\Ren, \Ren))$.
  
  Wir zeigen die Behauptung induktiv über die quadratische Konvergenz.
  Da $\D f(x^{*})$ invertierbar ist und $f\in C^2(\Omega) $,
  existiert nach dem Satz über implizite Funktionen
  eine Umgebung $\overline{B_\varepsilon(x^{*})}\subset \Omega$,
  auf der $\D f(x)$ invertierbar und stetig ist. Sei 
  \begin{gather*}
    c\coloneqq \sup_{x\in B_\varepsilon(x^{*})} \nn[[\D f(x)]^{-1}]
  \end{gather*}
  und 
  \begin{gather*}
    w\coloneqq \sup_{x\in B_\varepsilon(x^{*})}\nn[D^2f(x)]
  \end{gather*}
  Für $x^{(k)}\in B_\varepsilon(x^{*}) $ ist 
  $x^{(k)}+t(x^{*}-x^{(k)})\in B_\varepsilon(x^{*})$
  für $t\in [0,1]$ und 
  \begin{gather*}
    h^{(k)}(t) \coloneqq f(x^{(k)}+ t(x^{*}-x^{(k)}))\qquad \forall t\in [0,1]
  \end{gather*}
  ist wohldefiniert und in $C^2([0,1], \Ren)$.
  Wie in \ref{5.4.2} folgt 
  \begin{align*}
    x^{(k+1)}-x^{*} 
    &= [\D f(x^{(k)})]^{-1}\left(
      f(x^{*})-f(x^{(k)})-\D f(x^{(k)})(x^{*}-x^{(k)})
      \right)\\
    &= [\D f(x^{(k)})]^{-1}\left( 
      h^{(k)}(1)-h^{(k)}(0)-Dh^{(k)}(0)\cdot 1
      \right)\\
    &= [\D f(x^{(k)})]^{-1}\int_{0}^{1}D^2h^{(k)}(1-t)dt 
    &&\text{(Restglieddarst. der Taylorentw.)}
  \end{align*}
  Das Ziel ist nun zu zeigen, dass $\nn[x^{(k+1)}-x^{*}] \leq c\cdot \nn[x^{(k)}-x^{*}]^2$.
  Mit den Definitionen von oben wird die Ungleichung zu
  \begin{align}\nonumber
    \nn[x^{(k+1)}-x^{*}]
    &\leq c\cdot \frac{1}{2} \sup_{t\in[0,2]} \nn[D^2h^{(k)}(t)] \\
    & \leq c\cdot \frac{1}{2} w\nn[x^{(k)}-x^{*}]^2
      \label{V.5.5}
  \end{align}
  und zwar für alle $x^{(k)}\in B_\varepsilon(x^{*})$, 
  wie noch gezeigt wird.
  Sei nun $\delta \leq \varepsilon$, 
  so dass $\frac{1}{2} \cdot c\cdot w <1$ gilt,
  so folgt induktiv für $x^{(0)}\in B_\delta(x^{*})$ mit \eqref{V.5.5}
  \begin{align*}
    \nn[x^{(k+1)}-x^{*}] &\leq \frac{1}{2}w\delta^2 < \delta\\
    \Rightarrow x^{(k+1)}&\in B_\delta(x^{*})\subseteq B_\varepsilon(x^{*})
  \end{align*}
  Auf $x^{(k+1)} $ ist der nächste Iterationsschritt anwendbar
  und mit \eqref{V.5.5} folgt quadratische Konvergenz.
  Es bleibt zu zeigen:
  \begin{gather*}
    \nn[D^2h(t)] \leq w\nn[x^{(k)}-x^{*}]^2 \qquad \forall t\in [0,1],~
    h:[0,1]\rightarrow\Ren,~ h=\begin{pmatrix} h_1 \\ \vdots \\ h_n \end{pmatrix}
  \end{gather*}
  Hierfür betrachte
  \begin{align*}
    Dh_i^{(k)}(t)
    &=\underbrace{
      \D f_i\left( x^{(k)}+t(x^{*}-x^{(k)})\right)
      }_{\in \R^{1\times n}}
      \cdot (x^{*}-x^{(k)})\in\R\\
    D^2h_i^{(k)}(t)
    &=(x^{*}-x^{(k)})^T\cdot
      \underbrace{
      D^2f_i\left( x^{(k)}+t(x^{*}-x^{(k)})\right)
      }_{\in \R^{n\times n}}
      \cdot (x^{*}-x^{(k)})\in\R\\
  \end{align*}
\end{proof}

Unter genaueren Voraussetzungen kann die Existenz von $x^{*}$ gezeigt 
und eine Umgebung $B_r(x^{*})$ explizit angegeben werden.

Dies liefert folgender Satz:
\begin{satz}[Satz von Kantorowitsch]
  Sei $f\colon\Omega_0\to \Ren$, $\Omega_0 \subset \Ren$ konvex,
  $f$ stetig differenzierbar auf $\Omega_0$ und 
  erfülle für ein $x^{(0)}\in \Omega_0$ folgendes:
  \begin{enumerate}[a)]
  \item $\nn[\D f(x) -\D f(y)]\leq \gamma \nn[x-y]$ für alle $x,y\in \Omega_0$
  \item $\nn[[\D f(x^{(0)})]^{_1}] \leq \beta$
  \item $\nn[[\D f(x^{(0)})]^{_1}f(x^{(0)})] \leq \alpha$
  \end{enumerate}
  mit den Konstanten $h=\alpha\beta\gamma$, 
  $r_\pm = \frac{1\pm \sqrt{1-2h}}{h}\alpha$.
  Dann hat $f$, falls $h\leq \frac{1}{2}$ 
  und $\overline{B_{r_{-}}(x^{(0)})}\subset \Omega$,
  genau eine Nullstelle $x^{*}$ in $\Omega_0\cap B_{r_+}(x^{(0)})$.
  Weiterhin bleibt die Folge der Newton-Iterierten in $B_{r_{-}}(x^{(0)})$
  und konvergiert gegen $x^{*}$.
  \begin{proof}
    z.B. in Ortega/Rheinhold (2000)
  \end{proof}
\end{satz}

\sectione{Abbruchkriterien beim Newton-Verfahren}
\begin{enumerate}[1)]
\item Limitiere die Anzahl der Iterationen, u.a. um 
  Endlosschleifen durch fehlerhafte Programme auszuschließen.
\item Breche ab, wenn das Verfahren nicht konvergiert, d.h.
  wenn $x^{(k)}$ nicht im Konvergenzbereich bleibt.
\item Breche ab, wenn das Ergebnis genau genug ist, d.h. der
  Fehler $e^{(k)}\coloneqq \nn[x^{*}-x^{(k)}]$ klein genug ist.
\end{enumerate}


\subsectione{Der Monotonietest}
Beim Newton-Verfahren sollte die Funktion $g$ der zugehörigen
Fixpunktiteration eine Kontraktion sein, d.h. es muss ein 
$\kappa \in (0,1)$ für alle $k$ geben mit
\begin{align}\nonumber
  \nn[\Delta x^{(k)}]
  &=\nn[x^{(k+1)}-x^{(k)}] \\ \nonumber
  &= \nn[g(x^{(k)})-g(x^{(k-1)})]\\
  &\leq \kappa\nn[x^{(k)}-x^{(k-1)}] = \kappa \nn[\Delta x^{(k-1)}]
    \label{V.6.1}
\end{align}
Als Abbruchkriterium für eine (mögliche) Divergenz des Verfahrens wähle z.B.
$\kappa=\frac{1}{2}$ und breche ab, falls 
\begin{gather}
  \nn[\Delta x^{(k)}]>\frac{1}{2}\nn[\Delta x^{(k-1)}]
  \label{V.6.2}
\end{gather}
Um im Mehrdimensionalen eine vielleicht unnötig (teure) Berechnung
von $\D f(x^{(k)})$ bzw. von $\Delta x^{(k)}$ zu vermeiden, 
kann $\Delta x^{(k)}$ durch 
\begin{gather}
  \overline{\Delta x}^{(k)} = -[\D f(x^{(k-1)})]^{-1}\cdot f(x^{(k)})
  \label{V.6.3}
\end{gather}
approximiert werden.
$\D f(x^{(k-1)})$ und eine Zerlegung liegt bereits 
aus der Berechnung von $\Delta x^{(k-1)}$ vor.
Ebenso ist $f(x^{(k)})$ bekannt.
Die Lösung von \eqref{V.6.3} benötigt daher nur $\mathcal{O}(n^2)$ flops.
Statt \eqref{V.6.2} kann dann auch auf 
\begin{gather}
  \nn[\overline{\Delta x}^{(k)}] \geq \frac{1}{2} \nn[\Delta x^{(k-1)}]
  \label{V.6.4}
\end{gather}
getestet werden.


\subsectione{Kriterium für erreichte Konvergenz}\index{Toleranz}
Es ist $f(x^{*})$ gesucht, also teste hierauf. Das residuumbasierte Kriterium
\begin{gather}
  \nn[f(x^{(k)})]\leq \Tol
  \label{V.6.5}
\end{gather}
ist nur bedingt anwendbar, 
denn nach \ref{5.5.5} ist das Verfahren affin-invariant.
Demnach bleibt $(x^{(k)})_{k\in\N}$ gleich,
ob nun $f(x)$ oder $\widetilde{f}(x) =\alpha f(x) $ betrachtet wird.
Aber für $\widetilde{f}$ bricht \eqref{V.6.5} das Verfahren ab, 
falls $|\alpha|\cdot \nn[f(x^{(k)})] \leq \Tol$. 
Affin-invariant ist dagegen der Ansatz
\begin{gather}
  \nn[\Delta x^{(k)}]= \nn[x^{(k+1)}-x^{(k)}] 
  = \nn[{[\D f(x^{(k)})]^{-1}f(x^{(k)})}] 
  \leq \Tol \, .
  \label{V.6.6}
\end{gather}
\eqref{V.6.6} kann aufgrund der quadratischen Konvergenz (nur) für 
große $k$ auch mit \eqref{V.3.5} der Approximation des Fehlers 
$\nn[x^{*}-x^{(k)}] $ motiviert werden.


\sectione{Varianten des Newton-Verfahrens}
$\D f(x^{(k)})$ steht nicht immer analytisch zur Verfügung.
Die exakte Jacobi-Matrix wird häufig durch eine andere Matrix $B$ approximiert, 
z.B. durch Differenzenquotienten oder sogenanntes
\enquote{automatisches Differenzieren}.
Der Iterationsschritt lautet dann
\begin{align}
  \text{löse}\quad B^{(k)}d^{(k)} &= -f(x^{(k)}) 
                                    \label{V.7.1} \\\nonumber
  x^{(k+1)} &=x^{(k)} + d^{(k)}
\end{align}
Um den Aufwand zu verringern kann $\D f(x^{(k)})$ durch
$\D f(x^{(0)})$ approximiert werden.


\subsectione{Iterationsschritt des vereinfachten Newton-Verfahrens}
\index{Newton-Verfahren!vereinfacht}
\begin{gather}
  x^{(k+1)} = x^{(k)} -[\D f(x^{(0)})]^{-1} f(x^{(k)})
  \label{V.7.2}
\end{gather}
Das Verfahren konvergiert nur noch lokal linear.
Der Aufwand je Iteration ist jedoch erheblich geringer.


% ----------------------------------------------------------------------------

\subsectione{Das Broyden-Verfahren}\index{Broyden-Verfahren}
\marginpar{01.12.2014}
Das Broyden-Verfahren ist eine Verallgemeinerung des Sekantenverfahrens
auf $n>1$. $\D f(x^{(k)})$ wird durch den
\enquote{Differenzenquotienten} approximiert, d.h.
\begin{gather}
  B^{(k)}(\underbrace{x^{(k)}-x^{(k-1)}}_{\coloneqq p^{(k-1)}})
  = \underbrace{f(x^{(k)})-f(x^{(k-1)})}_{\coloneqq
    q^{(k-1)}}
  \label{V.7.3}
\end{gather}
$B^{(k)}$ ist jedoch nicht eindeutig durch \eqref{V.7.3} festgelegt.
Das Broyden-Verfahren bestimmt $B^{(k)}$ rekursiv durch eine 
Aufdatierung mit einer Rang-1-Matrix, auch \enquote{rang-1-update}
($C_\text{neu} = C_\text{alt} +M$ mit $\Rang(M)=1$). \\

Ein Iterationsschritt des Broyden-Verfahrens ist 
\begin{align}\nonumber
  d^{(k)} &= -[B^{(k)}]^{-1} f(x^{(k)}) \\\nonumber
  x^{(k+1)} &= x^{(k)} + d^{(k)} \\\nonumber
  p^{(k)} & \coloneqq d^{(k)} \qquad \text{nach
            \eqref{V.7.1}}\\\nonumber
  q^{(k)} & \coloneqq f(x^{(k+1)})-f(x^{(k)}) \\
  B^{(k+1)} & = B^{(k)} + \frac{1}{{p^{(k)}}^T\cdot p^{(k)}}
              \cdot \left(q^{(k)}-
              \underbrace{B^{(k)}p^{(k)}}_{\substack{=-f(x^{(k)})\\
  \text{ nach \eqref{V.7.1}}}}
  \right){p^{(k)}}^T
  \label{V.7.4}
\end{align}
Hierfür muss $x^{(0)} $ und $B^{(0)}$ gegeben sein.
Unter bestimmten Voraussetzungen konvergiert das Verfahren lokal
superlinear \cite[siehe][dortige Referenzen]{stoerbulirsch}.
Der fleißige Leser vergewissere sich, dass für
\eqref{V.7.4} auch \eqref{V.7.3} gilt.


\subsectione{Das gedämpfte Newton-Verfahren}
Es gilt
\begin{align}
  f(x^{*}) &= 0 
  & \Longleftrightarrow & 
  & f(x^{*}) =\min_{x\in\Ren} \frac{1}{2} \nn[f(x)]_2^2
  \label{V.7.5}
\end{align}
Betrachte nun die Funktion $\Phi\colon\Ren\to \R$ mit 
\begin{gather*}
  \Phi (x) \coloneqq \frac{1}{2} \nn[f(x)]_2^2 
  = \frac{1}{2} f(x)^T f(x)
\end{gather*}
Für $\Phi$ ist die Newton-Korrektur
$d^{(k)} \coloneqq \Delta x^{(k)} \coloneqq -[\D f(x)]^{-1}f(x)$
in $x^{(k)} $ eine \textbf{Abstiegsrichtung},
d.h. für $\mu >0 $ klein genug gilt
\begin{gather}
  \Phi(x^{(k)}+\mu d^{(k)}) < \Phi(x^{(k)} )
  \label{V.7.6}
\end{gather}
denn für $ f(x)\neq 0$
\begin{align*}
  \left.\frac{\dd}{\dd\mu} \Phi(x+\mu d)\right\vert_{\mu = 0} 
  &= \left[f(x+\mu d)^T\D f(x+\mu d)d\right]_{\mu = 0}\\
  &= -f(x)^Tf(x) \\
  &< 0
\end{align*}
Die Idee ist nun, statt $\mu = 1$ wie im Newton-Verfahren
ein \enquote{geeignetes} $\mu \in (0,1]$ zu wählen und 
\begin{gather}
  x^{(k+1)} = x^{(k)} +\mu d^{(k)}
  \label{V.7.7}
\end{gather}
entsprechend zu setzen, d.h. dämpfe $d$ mit Schrittweite $\mu$.
Ein möglicher \enquote{Eignungstest} ist
\begin{gather}
  \nn[f(x^{(k)}+\mu d^{(k)})]
  \leq (1-\frac{1}{2}\mu )\nn[f(x^{(k)})]
  \label{V.7.8}
\end{gather}
Und eine mögliche Strategie um $\mu $ zu bestimmen ist 
\begin{enumerate}[1.]
\item Setze $\mu=1$.
\item Halbiere $\mu$ rekursiv solange, bis \eqref{V.7.8} gilt.
\end{enumerate}
Es sind allerdings effektivere Dämpfungsstrategien bekannt!

Es gibt also eine äußere Iteration $(k)$
um $x^{*}$ zu bestimmen und eine Innere,
um für jedes $(k)$ ein geeignetes $\mu$ zu berechnen.
Die innere Schleife ist mit $n$ eindimensionalen Funktionsauswertungen
\enquote{billig}.
Unter bestimmten Voraussetzungen ist
\textbf{globale} Konvergenz gewährleistet.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../numerik_script"
%%% End:
